\section{Steganography and Large Language Models}
\label{sec:llm_approaches}

Large Language Models (LLMs) have emerged as a significant development in the field of natural language processing, profoundly impacting text generation and related applications like steganography and watermarking. Here's a breakdown of their emergence and impact:

\subsection{Capabilities and Approximating Natural Communication}
LLMs are \textbf{generative models} that can \textbf{approximate complex distributions like text-based communication}. They represent the best-known technique for this task.
These models operate by taking context and parameters to output an explicit probability distribution over the next token (e.g., a character or a word). The next token is typically sampled randomly from this distribution, and the process repeats to generate output of a desired length.
Training LLMs involves processing vast amounts of data to set parameters and structure, enabling their output distributions to approximate true distributions in the training data.
The \textbf{quality of content generated by generative models is impressive} and continues to improve. This has led to LLMs blurring the boundary of high-quality text generation between humans and machines.
LLMs are increasingly used to generate data for specific tasks, such as tabular data, relational triples, sentence pairs, and instruction data, often achieving satisfactory generation quality in zero-shot learning for specific subject categories.
They have also shown capabilities in mimicking language styles and semantics, and their generalization ability allows them to comprehend the semantics of context.

\subsection{Role in Generative Linguistic Steganography}
LLMs are considered \textbf{favorable for generative text steganography} due to their ability to generate high-quality text.
Researchers propose using generative models as steganographic samplers to embed messages into realistic communication distributions, such as text. This is a departure from prior steganographic work and is motivated by the public availability of high-quality models and significant efficiency gains.
LLMs like \textbf{GPT-2, LLaMA, and Baichuan2} are commonly used as basic generative models for steganography.
Existing methods often use a language model and steganographic mapping, where secret messages are embedded by establishing a mapping between binary bits and the sampling probability of words within the training vocabulary.
However, traditional "white-box" methods require sharing the exact language model and training vocabulary, which limits fluency, logic, and diversity compared to natural texts generated by LLMs. They also inevitably change the sampling probability distribution, posing security risks.
New approaches, like \textbf{LLM-Stega}, explore \textbf{black-box generative text steganography using the user interfaces (UIs) of LLMs}, overcoming the need to access internal sampling distributions. This method constructs a keyword set and uses an encrypted steganographic mapping for embedding, proposing an optimization mechanism based on reject sampling for accurate extraction and rich semantics.
Another framework, \textbf{Co-Stega}, leverages LLMs to address the low capacity challenge in social media by increasing the text space for hiding messages (through context retrieval) and \textbf{raising the generated text's entropy via specific prompts} to increase embedding capacity. This approach also aims to maintain text quality, fluency, and relevance.
The concept of \textbf{zero-shot linguistic steganography} with LLMs utilizes in-context learning, where samples of covertext are used as context to generate more intelligible stegotext using a question-answer (QA) paradigm.
LLMs are also used in approaches like \textbf{ALiSa}, which directly conceals token-level secret messages in seemingly natural steganographic text generated by off-the-shelf BERT models equipped with Gibbs sampling.
The increasing popularity of deep generative models has made it feasible for provably secure steganography to be applied in real-world scenarios, as they fulfill requirements for perfect samplers and explicit data distributions.



\subsection{LLM-Based Steganography Models}



\subsubsection{Evaluation Metrics}

\paragraph{Imperceptibility Metrics}
Perceptual: PPL, Distinct-n, MAUVE, human evaluation. Statistical: KLD, JSD, anti-steganalysis accuracy, semantic similarity.



\paragraph{Embedding Capacity Metrics}
Bits per token/word, embedding rate.

\subsection{Challenges and Limitations in Steganography with LLMs}

\subsubsection{Perceptual vs. Statistical Imperceptibility (Psic Effect)}
Improving perceptual quality can reduce statistical security, and vice versa.

\subsubsection{Low Embedding Capacity}
Short texts and strict semantics limit how much information can be hidden.

\subsubsection{Lack of Semantic Control and Contextual Consistency}
Ensuring generated text matches intended meaning/context is difficult.

\subsubsection{Challenges with LLMs in Steganography}
LLMs may introduce unpredictability, bias, or leak information.

\subsubsection{Segmentation Ambiguity}
Tokenization can cause ambiguity in how information is embedded or extracted.

Despite their capabilities, generative models are still \textbf{far from perfect} in imitating real communication.
A significant challenge for practical steganography is the difficulty of finding samplers for non-trivial distributions like the English language, which continues to evolve.
When using approximate samplers, there's a risk that an adversary can detect a steganographic message by distinguishing between the real channel and the approximation.
LLMs are known to make mistakes, including "hallucinations," which can lead to errors and erratic embedding during text generation, especially for long stego sequences.
One critical issue is \textbf{segmentation ambiguity} in neural linguistic steganography. LLMs often use \textbf{subword tokenization}, meaning a single text can correspond to multiple token representations. If the sender and receiver have different understandings of segmentation, it can lead to incorrect message extraction and affect subsequent generation steps. Current provably secure methods have largely overlooked this. SyncPool is a proposed method to address this by grouping tokens with prefix relationships in the candidate pool without altering the original probability distribution.
The \textbf{computational overhead of LLMs is higher} compared to prior methods (approximately 3x to 5x), potentially limiting real-time communication.
The effectiveness of LLM-based steganography can be limited by the \textbf{entropy of the cover text} in social media contexts, as short, context-dependent replies have lower entropy, thus limiting hiding capacity.
