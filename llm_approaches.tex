\section{Steganography and Large Language Models}
\label{sec:llm_approaches}

Large Language Models (LLMs) have emerged as a significant development in the field of natural language processing, profoundly impacting text generation and related applications like steganography and watermarking. Here's a breakdown of their emergence and impact:

\subsection{Capabilities and Approximating Natural Communication}
LLMs are \textbf{generative models} that can \textbf{approximate complex distributions like text-based communication}. They represent the best-known technique for this task.
These models operate by taking context and parameters to output an explicit probability distribution over the next token (e.g., a character or a word). The next token is typically sampled randomly from this distribution, and the process repeats to generate output of a desired length.
Training LLMs involves processing vast amounts of data to set parameters and structure, enabling their output distributions to approximate true distributions in the training data.
The \textbf{quality of content generated by generative models is impressive} and continues to improve. This has led to LLMs blurring the boundary of high-quality text generation between humans and machines.
LLMs are increasingly used to generate data for specific tasks, such as tabular data, relational triples, sentence pairs, and instruction data, often achieving satisfactory generation quality in zero-shot learning for specific subject categories.
They have also shown capabilities in mimicking language styles and semantics, and their generalization ability allows them to comprehend the semantics of context.

\subsection{Role in Generative Linguistic Steganography}
LLMs are considered \textbf{favorable for generative text steganography} due to their ability to generate high-quality text.
Researchers propose using generative models as steganographic samplers to embed messages into realistic communication distributions, such as text. This is a departure from prior steganographic work and is motivated by the public availability of high-quality models and significant efficiency gains.
LLMs like \textbf{GPT-2, LLaMA, and Baichuan2} are commonly used as basic generative models for steganography.
Existing methods often use a language model and steganographic mapping, where secret messages are embedded by establishing a mapping between binary bits and the sampling probability of words within the training vocabulary.
However, traditional "white-box" methods require sharing the exact language model and training vocabulary, which limits fluency, logic, and diversity compared to natural texts generated by LLMs. They also inevitably change the sampling probability distribution, posing security risks.
New approaches, like \textbf{LLM-Stega}, explore \textbf{black-box generative text steganography using the user interfaces (UIs) of LLMs}, overcoming the need to access internal sampling distributions. This method constructs a keyword set and uses an encrypted steganographic mapping for embedding, proposing an optimization mechanism based on reject sampling for accurate extraction and rich semantics \cite{wu2024generative}.
Another framework, \textbf{Co-Stega}, leverages LLMs to address the low capacity challenge in social media by increasing the text space for hiding messages (through context retrieval) and \textbf{raising the generated text's entropy via specific prompts} to increase embedding capacity. This approach also aims to maintain text quality, fluency, and relevance \cite{liao2024co}.
The concept of \textbf{zero-shot linguistic steganography} with LLMs utilizes in-context learning, where samples of covertext are used as context to generate more intelligible stegotext using a question-answer (QA) paradigm \cite{lin2024zero}.
LLMs are also used in approaches like \textbf{ALiSa}, which directly conceals token-level secret messages in seemingly natural steganographic text generated by off-the-shelf BERT models equipped with Gibbs sampling \cite{yi2022alisa}.
The increasing popularity of deep generative models has made it feasible for provably secure steganography to be applied in real-world scenarios, as they fulfill requirements for perfect samplers and explicit data distributions \cite{ding2023discop, kaptchuk2021meteor, qi2024provably}.



\subsection{LLM-Based Steganography Models}



\subsubsection{Evaluation Metrics}

\paragraph{Imperceptibility Metrics}
Perceptual: PPL, Distinct-n, MAUVE, human evaluation. Statistical: KLD, JSD, anti-steganalysis accuracy, semantic similarity.



\paragraph{Embedding Capacity Metrics}
Bits per token/word, embedding rate.

\subsection{Challenges and Limitations in Steganography with LLMs}

\subsubsection{Perceptual vs. Statistical Imperceptibility (Psic Effect)}
Improving perceptual quality can reduce statistical security, and vice versa.

\subsubsection{Low Embedding Capacity}
Short texts and strict semantics limit how much information can be hidden.

\subsubsection{Lack of Semantic Control and Contextual Consistency}
Ensuring generated text matches intended meaning/context is difficult.

\subsubsection{Challenges with LLMs in Steganography}
LLMs may introduce unpredictability, bias, or leak information.

\subsubsection{Segmentation Ambiguity}
Tokenization can cause ambiguity in how information is embedded or extracted.

A primary challenge in steganography, particularly when utilizing Large Language Models (LLMs), revolves around the \textbf{distinction between white-box and black-box access}. Most current advanced generative text steganographic methods operate under a "white-box" paradigm, meaning they require direct access to the LLM's internal components, such as its training vocabulary and the sampling probabilities of words. This presents a significant limitation because many state-of-the-art LLMs are proprietary and are accessed by users primarily through black-box APIs or user interfaces \cite{wu2024generative}. Consequently, these white-box methods are often impractical for real-world deployment with popular commercial LLMs. Furthermore, methods that rely on modifying the sampling probability distribution to embed secret messages inherently introduce security risks because they alter the original distribution, making the steganographic text statistically distinguishable from normal text \cite{yang2020vae, kaptchuk2021meteor, ding2023discop, wu2024generative}.

Another significant hurdle is \textbf{ensuring both the quality and imperceptibility of the generated text}, encompassing perceptual, statistical, and cognitive imperceptibility. While advancements in deep neural networks have improved text fluency and embedding capacity, older models or certain embedding strategies can still produce texts that lack naturalness, logical coherence, or diversity compared to human-written content. Linguistic steganography methods often struggle to control the semantics and contextual characteristics of the generated text, leading to a decline in its "cognitive-imperceptibility" \cite{yang2020vae, ding2023context}. This can make concealed messages easier for human or machine supervisors to detect. Although models like NMT-Stega and Hi-Stega aim to maintain semantic and contextual consistency by leveraging source texts or social media contexts, this remains a complex challenge \cite{ding2023context, wang2023hi}.

\textbf{Channel entropy requirements and variability} also pose a considerable challenge. Traditional universal steganographic schemes often demand that the communication channel maintains a minimum level of entropy, which is rarely consistent in real-world communication, especially in natural language. Moments of low or zero entropy can cause existing steganographic protocols to fail or necessitate the generation of extraordinarily long steganographic texts, making covert communication impractical. While schemes like Meteor attempt to adapt by fluidly changing the encoding rate proportional to instantaneous entropy, overcoming this variability without increasing detectability is difficult. The "Psic Effect" (Perceptual-Statistical Imperceptibility Conflict Effect) highlights this dilemma, where optimizing for perceived quality might compromise statistical imperceptibility and vice-versa.

Furthermore, \textbf{segmentation ambiguity} introduced by subword-based language models, commonly used in high-performing Transformer architectures, presents a critical issue for provably secure linguistic steganography. When a sender detokenizes generated subword sequences into a continuous text (e.g., "any" + "thing" becoming "anything") before transmission, the receiver might retokenize it differently (e.g., as a single "anything" token), leading to decoding errors and affecting subsequent probability distributions. Existing disambiguation solutions typically involve modifying the token candidate pool or probability distributions, which renders them incompatible with the strict requirements of provably secure steganography that demand unchanged distributions \cite{qi2024provably}. While SyncPool attempts to address this without altering the distribution, it may still lead to a reduction in the embedding rate due to information loss \cite{qi2024provably}.

Additional limitations include:
*   \textbf{Computational Overhead}: LLMs, while powerful, incur a higher computational cost (3-5 times more than prior methods), which could impact real-time communication scenarios \cite{lin2024zero}.
*   \textbf{Data Integrity and Reversibility}: Some linguistic steganography methods are not reversible, meaning the original cover text cannot be perfectly recovered after message extraction, which is undesirable for sensitive applications \cite{zheng2022general, qiang2023natural}. Text data is generally less prone to lossy compression issues than other media, but incompleteness of the steganographic text can still damage the embedded bitstream \cite{lin2024zero}.
*   \textbf{Ethical Concerns}: The use of pre-trained LLMs may inadvertently introduce ethical issues such as political biases, gender discrimination, or the generation of insulting content \cite{lin2024zero}.
*   \textbf{Provable Security and Rigor}: Despite decades of research into provably secure steganography, practical systems have been hampered by strict requirements like perfect samplers and explicit data distributions \cite{ding2023discop, kaptchuk2021meteor}. Many works from the NLP community, while generating convincing text, often lack rigorous security analyses and fail to meet formal cryptographic definitions, making them vulnerable to detection \cite{kaptchuk2021meteor}.

Despite their capabilities, generative models are still \textbf{far from perfect} in imitating real communication.
A significant challenge for practical steganography is the difficulty of finding samplers for non-trivial distributions like the English language, which continues to evolve.
When using approximate samplers, there's a risk that an adversary can detect a steganographic message by distinguishing between the real channel and the approximation \cite{kaptchuk2021meteor}.
LLMs are known to make mistakes, including "hallucinations," which can lead to errors and erratic embedding during text generation, especially for long stego sequences.
One critical issue is \textbf{segmentation ambiguity} in neural linguistic steganography. LLMs often use \textbf{subword tokenization}, meaning a single text can correspond to multiple token representations. If the sender and receiver have different understandings of segmentation, it can lead to incorrect message extraction and affect subsequent generation steps. Current provably secure methods have largely overlooked this. SyncPool is a proposed method to address this by grouping tokens with prefix relationships in the candidate pool without altering the original probability distribution.
The \textbf{computational overhead of LLMs is higher} compared to prior methods (approximately 3x to 5x), potentially limiting real-time communication.
The effectiveness of LLM-based steganography can be limited by the \textbf{entropy of the cover text} in social media contexts, as short, context-dependent replies have lower entropy, thus limiting hiding capacity \cite{liao2024co}.
