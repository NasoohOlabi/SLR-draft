\section{Introduction}
\label{sec:introduction}

Linguistic steganography, the practice of concealing information within natural language text, has long been regarded as one of the most challenging areas of covert communication due to the low redundancy \cite{yang2020vae} \cite{kaptchuk2021meteor}, semantic rigidity, and statistical sensitivity of language. Traditional methods —such as synonym substitution, syntactic transformations, or rule-based embedding— often suffer from limited capacity and detectability \cite{7925203}, making them inadequate against modern steganalysis. The emergence of large language models (LLMs), however, has profoundly transformed this landscape by enabling the generation of coherent, context-aware, and statistically natural covertexts \cite{10650062}, thereby providing a foundation for high-capacity and imperceptible covert communication. The field has seen the emergence of various LLM-based steganography paradigms: generative methods that directly create stego texts \cite{yang2020vae}\cite{DBLP:journals/corr/abs-2106-02011}\cite{ding2023discop}\cite{wu2024generative}, rewriting-based methods that rephrase existing cover texts \cite{li2023rewriting}, black-box approaches that utilize LLM user interfaces or APIs without needing access to internal model parameters \cite{wu2024generative}\cite{steinebach2024natural}, zero-shot methods that leverage in-context learning in contrast to fine tuning with LLMs to generate intelligible stego text \cite{lin2024zero}, collaborative frameworks that exploit contextual relevance within social media or combine retrieval and generation strategies to expand embedding space and enhance entropy \cite{liao2024co}\cite{wang2023hi}, provably secure methods that focus on mathematically rigorous security definitions, achieving indistinguishability from honest model output \cite{kaptchuk2021meteor}\cite{ding2023discop}. While LLMs offer significant advantages, challenges like the "Psic Effect" (a trade-off between text quality and statistical imperceptibility) \cite{yang2020vae}, computational overhead, and segmentation ambiguity still present areas for ongoing research. This paper presents a systematic literature review that synthesizes recent advances in LLM-based linguistic steganography, identifies unresolved challenges, and highlights future research directions.

Previous reviews on text steganography, such as the one by Majeed et al. (2021) \cite{math9212829}, primarily focus on older techniques and were published before the widespread adoption of Large Language Model (LLM)-based approaches. While the more recent review by Setiadi et al. (2025) \cite{Setiadi_Ghosal_Sahu_2025} acknowledges that the field of linguistic steganography "has been revitalized by large language models (LLMs)" and specifically examines recent AI-powered steganography methods from the last three years (post-2021), detailing techniques that utilize models like GPT-2 \cite{radford2019gpt2}, GPT-3 \cite{brown2020languagemodelsfewshotlearners}, LLaMA2 \cite{touvron2023llama2openfoundation}, and Baichuan2 \cite{xiao2024baichuan2suminstructionfinetunebaichuan27b}, it is important to note that the Setiadi et al. (2025) review is not a systematic literature review. It's a "concise and critical examination" rather than an exhaustive survey, it does not include all relevant papers published between 2021 and 2025.
Consequently, despite the advancements discussed, a notable gap persists for a comprehensive systematic literature review that fully summarizes how large-scale transformers have reshaped text steganography. This is in contrast to earlier surveys that predominantly identified classical approaches such as synonym replacement, spacing, and Huffman coding, which predated the LLM revolution \cite{math9212829}. 

% Furthermore, the field faces significant challenges in evaluation standardization that compound the need for systematic analysis. While core metrics like embedding rate (ER), Kullback-Leibler divergence (KLD), and perplexity (PPL) are consistently used across studies, their inconsistent application hinders meaningful cross-method comparisons. For instance, PPL calculations vary depending on the underlying language model used (GPT-2, LLaMA, etc.) and text length, KLD measurements differ based on the reference datasets employed, and ER reporting lacks uniformity—with some studies measuring bits per token while others use bits per word. This inconsistency is compounded by the use of heterogeneous datasets across studies, ranging from IMDb and BookCorpus to specialized corpora like News-Commentary-v13 and HC3. Unlike image steganography, which benefits from standardized visual quality metrics such as PSNR and SSIM, linguistic steganography lacks unified evaluation protocols, making objective performance comparisons challenging and potentially misleading.
Furthermore, the field faces significant challenges in evaluation standardization 
% [citation needed]
 that compound the need for systematic analysis. While core metrics like embedding rate (ER) \cite{10.1007/3-540-49380-8_21}
, Kullback-Leibler divergence (KLD) \cite{1320776d-9e76-337e-a755-73010b6e4b64}
, and perplexity (PPL) \cite{10.1121/1.2016299}
 are consistently used across studies, their inconsistent application hinders meaningful cross-method comparisons. For instance, PPL calculations vary depending on the underlying language model used (GPT-2, LLaMA, etc.) and the generated text length, KLD measurements differ based on the reference datasets (normal text) employed, and ER reporting lacks uniformity with some studies measuring bits per token while others use bits per word. This inconsistency is compounded by the use of heterogeneous datasets across studies, ranging from IMDb \cite{10.5555/2002472.2002491} and BookCorpus \cite{Zhu_2015_ICCV} to specialized corpora like News-Commentary-v13 [define/reference needed] and HC3 [define/reference needed]. Unlike image steganography, which benefits from standardized visual quality metrics such as PSNR [define/reference needed] and SSIM [define/reference needed], linguistic steganography [define/reference needed] lacks unified evaluation protocols, making objective performance comparisons challenging and potentially misleading [citation needed].

This systematic review fills these gaps by meticulously identifying and synthesizing recent primary literature that leverages LLMs for textual steganography, particularly from the last two years when LLMs like GPT-3/4 and open models became widely available. The timing is well-justified by the significant surge in publications and novel ideas since 2023, with approximately 70\% of recent studies using open-source LLMs like GPT-2, LLaMA2, and LLaMA3. The importance of this review is underscored by the transformative impact of LLMs on secure communication, marking a paradigm shift toward context-aware, generative systems that prioritize imperceptibility, embedding capacity, and naturalness. LLM-based steganography offers striking gains in classic metrics like capacity and imperceptibility; for instance, reviewed studies report that advanced white-box LLM samplers can achieve perplexities as low as 3-8 (on GPT-2 models) while embedding up to approximately 5.98 bits per token, far exceeding pre-LLM schemes. This enables secure clandestine messaging in environments where classical steganography was too limited or suspicious.

The rest of this paper follows a standard SLR structure. Section 2 provides background on steganography and LLMs, defining key concepts such as imperceptibility. Section 3 describes the scope and research questions. Section 4 details the literature search and selection methodology. Sections 5 and 6 present the data extraction process and classification of the selected studies. Section 7 reports the results organized by research question, summarizing state-of-the-art techniques, application domains, evaluation metrics, attack models, and the role of external knowledge sources. Finally, Section 8 synthesizes the main findings and discusses trends, and Section 9 concludes by outlining open problems and future research directions.