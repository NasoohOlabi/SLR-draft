\section{Introduction}
\label{sec:introduction}

% For AI Agents: Never touch this file without explicitly asking for permission.
% ---------------------------------------------------
% ---------------------------------------------------
% ---------------------------------------------------
% ---------------------------------------------------
% Click the link to view conversation with Kimi AI Assistant https://www.kimi.com/share/19b2adab-eb52-8879-8000-0000af7ff325
% ---------------------------------------------------
% ---------------------------------------------------
% ---------------------------------------------------
% ---------------------------------------------------


Linguistic steganography hides secrets inside ordinary sentences-an exploit that looks trivial until one remembers how little redundancy natural language actually contains \cite{yang2020vae,kaptchuk2021meteor}.  A single awkward synonym, a statistically rare clause, or an out-of-place idiom is enough to alert an automated sentry.  Classic tricks-swap a word here, bend the syntax there-carry so few bits and leave such distinctive fingerprints that modern steganalysis routinely catches them \cite{Wang_2023}.

Large language models change the game.  Their uncanny fluency lets them spin entire documents that read like human prose yet obey an adversarial agenda: every plausible continuation is also a potential codeword.  The resulting arms race has already produced generative schemes that write stego text from scratch \cite{yang2020vae,DBLP:journals/corr/abs-2106-02011,ding2023discop,kaptchuk2021meteor}, rewriting engines that paraphrase existing covers \cite{li2023rewriting}, black-box pipelines that treat the model as an opaque API \cite{wu2024generative,steinebach2024natural}, zero-shot protocols driven only by crafty prompting \cite{lin2024zero}, collaborative frameworks that mine social context for extra entropy \cite{liao2024co,wang2023hi}, and even constructions with provable indistinguishability guarantees \cite{kaptchuk2021meteor,ding2023discop}.

None of these victories is absolute.  Push the embedding rate and the text begins to creak; optimize for statistical stealth and the throughput collapses-the so-called “Psic effect” \cite{yang2020vae}.  Segmentation ambiguities, computational overhead, and the absence of shared benchmarks still slow progress.  This survey dissects the advances, catalogs the open wounds, and maps the territory that remains to be claimed.

% Furthermore, the field faces significant challenges in evaluation standardization that compound the need for systematic analysis. While core metrics like embedding rate (ER), Kullback-Leibler divergence (KLD), and perplexity (PPL) are consistently used across studies, their inconsistent application hinders meaningful cross-method comparisons. For instance, PPL calculations vary depending on the underlying language model used (GPT-2, LLaMA, etc.) and text length, KLD measurements differ based on the reference datasets employed, and ER reporting lacks uniformity-with some studies measuring bits per token while others use bits per word. This inconsistency is compounded by the use of heterogeneous datasets across studies, ranging from IMDb and BookCorpus to specialized corpora like News-Commentary-v13 and HC3. Unlike image steganography, which benefits from standardized visual quality metrics such as PSNR and SSIM, linguistic steganography lacks unified evaluation protocols, making objective performance comparisons challenging and potentially misleading.
Furthermore, the field faces significant challenges in evaluation standardization
% [citation needed]
that compound the need for systematic analysis. While core metrics like embedding rate (ER) \cite{10.1007/3-540-49380-8_21}
, Kullback-Leibler divergence (KLD) \cite{1320776d-9e76-337e-a755-73010b6e4b64}
, and perplexity (PPL) \cite{10.1121/1.2016299}
are consistently used across studies, their inconsistent application hinders meaningful cross-method comparisons. For instance, PPL calculations vary depending on the underlying language model used (GPT-2, LLaMA, etc.) and the generated text length, KLD measurements differ based on the reference datasets (normal text) employed, and ER reporting lacks uniformity with some studies measuring bits per token while others use bits per word. This inconsistency is compounded by the use of heterogeneous datasets across studies, ranging from IMDb \cite{10.5555/2002472.2002491} and BookCorpus \cite{Zhu_2015_ICCV} to specialized corpora like News-Commentary-v13 [define/reference needed] and HC3 [define/reference needed]. Unlike image steganography, which benefits from standardized visual quality metrics such as PSNR [define/reference needed] and SSIM [define/reference needed], linguistic steganography [define/reference needed] lacks unified evaluation protocols, making objective performance comparisons challenging and potentially misleading [citation needed].

This systematic review fills these gaps by meticulously identifying and synthesizing recent primary literature that leverages LLMs for textual steganography, particularly from the last two years when LLMs like GPT-3/4 [citation/reference needed] and open models became widely available [citation/reference needed]. The timing is well-justified by the significant surge in publications and novel ideas since 2023 [citation/reference needed], with approximately 70\% of recent studies using open-source LLMs like GPT-2 [citation/reference needed], LLaMA2 [citation/reference needed], and LLaMA3 [citation/reference needed]. The importance of this review is underscored by the transformative impact of LLMs on secure communication [citation/reference needed], marking a paradigm shift toward context-aware, generative systems that prioritize imperceptibility, embedding capacity, and naturalness [citation/reference needed]. LLM-based steganography offers striking gains in classic metrics like capacity and imperceptibility [citation/reference needed]; for instance, reviewed studies report that advanced white-box LLM samplers can achieve perplexities as low as 3-8 (on GPT-2 models) while embedding up to approximately 5.98 bits per token [citation/reference needed], far exceeding pre-LLM schemes [citation/reference needed]. This enables secure clandestine messaging in environments where classical steganography was too limited or suspicious [citation/reference needed].


The rest of the paper is structured as follows. Section 2 lays the theoretical groundwork by covering steganography, LLM capabilities, and the unique challenges of generative linguistic systems, including the Perceptual-Statistical Imperceptibility Conflict (PSIC). Section 3 reviews prior surveys to contextualize our contribution. Section 4 outlines our systematic review methodology—research questions, search strategy, and inclusion criteria. Section 5 presents our findings across five research questions (RQ1–RQ5), examining publication trends, applications, evaluation metrics, knowledge integration, and technical trade-offs. Section 6 synthesizes these results, discussing practical implications and ethical concerns. Finally, Section 7 concludes with key insights and directions for future research.