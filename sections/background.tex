\section{Context awareness}
\label{sec:background}

Linguistic steganography has evolved from early methods to advanced deep learning models and Large Language Models (LLMs). This progression focuses on improving imperceptibility, embedding capacity, and maintaining naturalness and semantic coherence in cover text.

\subsection{Context life cycle}

\subsection{Context-awareness in Steganography}

Here's an overview of this evolution:
\begin{itemize}
    \item \textbf{Early and Format-Based Approaches} Early steganography modified existing carriers, like using whitespace or linguistic idiosyncrasies (e.g., synonym substitution). These methods, often rule-based and context-neglecting, resulted in unnatural text and limited capacity (typically <1 BPT), making them easily detectable.
    \item \textbf{Transition to Carrier Generation and Early Text Generation Models} A significant shift involved "carrier generation based steganography," where the carrier text is generated to hide information, allowing greater freedom and higher embedding rates without altering original carrier statistics.
    \begin{itemize}
        \item \textbf{Syntax Rules and Statistical Methods:} Early text generation for steganography, using syntax rules or statistical models like Markov models, produced easily recognizable texts, failing imperceptibility and security.
        \item \textbf{Challenges of Early Generation:} The Psic Effect (perceptual-imperceptibility vs. statistical-imperceptibility conflict) was a key challenge, where attempts to make generated text appear natural to human observers often introduced statistical anomalies detectable by automated analysis. Generated text, though natural-looking, often had detectable statistical properties. Models also lacked semantic control, crucial for covert communication.
    \end{itemize}
    \item \textbf{The Era of Custom Artificial Neural Network (ANN) Models} Advancements in ANNs and NLP led to sophisticated models for automatic steganographic text generation. These neural networks learned language models, encoding secret information by manipulating word probability distributions during generation.

\end{itemize}