\section{Background}
\label{sec:background}

This section establishes the theoretical foundations for understanding LLM-based linguistic steganography. We first define steganography and its distinction from encryption, then examine why text is a challenging carrier medium. We then introduce the three dimensions of imperceptibility that guide evaluation, followed by theoretical limits based on channel entropy and perfect samplers. Finally, we introduce the concept of contextual compatibility, which serves as a core organizing principle for this review.

\subsection{Fundamentals of Steganography and Text as a Channel}

Information security systems broadly encompass \textbf{encryption}, \textbf{privacy}, and \textbf{concealment}, the last of which—known as \textbf{steganography}—is the focus of this review. While encryption and privacy protect message content, they do not conceal the existence of communication, which may itself arouse suspicion. Steganography instead prioritizes \textbf{imperceptibility}: embedding information into ordinary carriers (e.g., images or text) so that hidden messages remain unnoticed.

The classical "Prisoners' Problem" \cite{simmons1984prisoners} illustrates the goal: two parties, Alice and Bob, must exchange hidden information without alerting a watchful adversary. Text is a particularly challenging carrier due to its low redundancy and strict semantic constraints. Textual steganography methods are typically divided into \textbf{format-based} approaches, which exploit layout or structural features, and \textbf{content-based} approaches, which modify linguistic form. Within the latter, early techniques such as \textbf{synonym substitution} embed bits by altering lexical choices, but suffer from low capacity and high detectability. More formally, \textbf{linguistic steganography} refers to concealing information in natural language by modifying or generating text while preserving fluency and meaning \cite{fridrich2009steganography}.

\subsection{Dimensions of Imperceptibility}

Evaluating steganographic systems requires considering multiple dimensions of imperceptibility, each addressing different detection threats:

\begin{itemize}
    \item \textbf{Perceptual imperceptibility}: The generated text appears natural to human readers, maintaining fluency, coherence, and stylistic consistency. This dimension addresses human-based detection and is typically measured through human evaluation or fluency metrics like perplexity (PPL).
    \item \textbf{Statistical imperceptibility}: The distribution of the steganographic text is indistinguishable from that of natural text, preventing detection through statistical analysis. This dimension addresses machine-based steganalysis and is measured through metrics like Kullback-Leibler divergence (KLD), Jensen-Shannon divergence (JSD), and anti-steganalysis accuracy.
    \item \textbf{Cognitive imperceptibility}: The generated text maintains semantic and contextual fidelity, ensuring that the meaning and communicative context align with expectations. This dimension addresses detection through semantic or contextual inconsistencies and is measured through semantic similarity metrics and domain-specific evaluations \cite{ding2023context}.
\end{itemize}

The \textbf{Psic Effect} (Perceptual-Statistical Imperceptibility Conflict) \cite{yang2020vae} highlights a fundamental trade-off: optimizing for perceptual fluency (e.g., selecting high-probability tokens) may undermine statistical security by making the text distribution distinguishable from natural text, while optimizing for statistical indistinguishability may reduce perceptual naturalness. This trade-off is central to understanding the limitations and design choices in LLM-based steganography, as systematically analyzed in Research Question 5 (Section~\ref{subsec:rq5}), where we find that methods achieving high capacity often face detection accuracy drops of 5-50\%.

\subsection{Theoretical Limits: Channel Entropy and Perfect Samplers}

A deeper theoretical perspective introduces \textbf{channel entropy}, which quantifies the information-carrying capacity of a given communication channel. Entropy sets the upper bound for embedding rates: higher entropy allows more hidden information without detection, while lower entropy restricts capacity. In linguistic steganography, the channel is the distribution over possible texts, and the entropy depends on the context, domain, and linguistic constraints.

Achieving the theoretical capacity bound securely requires \textbf{perfect samplers}, which can generate text indistinguishable from genuine distributional samples. These concepts underpin the design of provably secure steganographic systems \cite{kaptchuk2021meteor, ding2023discop}. Large Language Models, with their ability to approximate high-dimensional distributions over natural language sequences, serve as powerful approximators for perfect samplers, enabling steganographic systems that approach theoretical capacity limits while maintaining imperceptibility.

However, real-world natural language communication rarely maintains consistent channel entropy. Moments of low or zero entropy (e.g., highly constrained contexts, formulaic expressions) can cause steganographic protocols to fail or require extraordinarily long texts. This variability in channel entropy is a key challenge addressed by context-aware steganographic systems, as explored in Research Question 4 (Section~\ref{subsec:rq4}), where we find that 65\% of studies incorporate external knowledge sources to enhance capacity by 15-25\% and improve contextual relevance.

\subsection{Contextual Compatibility and Context Handling}

A core organizing principle for this review is \textbf{contextual compatibility}: the degree to which a steganographic system generates text that is appropriate for its intended communicative context. Contextual compatibility encompasses semantic coherence, domain appropriateness, stylistic consistency, and alignment with the communicative purpose (e.g., social media posts, formal documents, technical documentation).

Methods handle context in different ways, which we classify as:
\begin{itemize}
    \item \textbf{Explicit context}: The method explicitly incorporates external context (e.g., source text, domain knowledge, social media context) into the generation process.
    \item \textbf{Implicit context}: The method leverages context that is inherent in the model's training or generation process without explicit external input.
    \item \textbf{No context}: The method generates text without explicit consideration of communicative context.
\end{itemize}

The representation of context also varies: it may be encoded as text (e.g., pretext, source documents), structured data (e.g., graphs, knowledge bases), or vector embeddings. How methods handle context directly impacts their capacity, imperceptibility, and applicability to different domains, as systematically analyzed in Research Question 4 (Section~\ref{subsec:rq4}), which reveals that explicit context methods achieve higher contextual relevance but may introduce 5-15\% computational overhead.

\subsection{Model Access Paradigms and Practical Constraints}

Model access further shapes practical steganography. With \textbf{black-box access} (e.g., commercial APIs), developers gain scalability and ease of use but face limited control over sampling distributions and reduced transparency. In contrast, \textbf{white-box access} enables fine-grained control over parameters and sampling, supporting stronger security guarantees and provable security, but requires costly resources and raises deployment barriers. \textbf{Hybrid approaches} combine elements of both paradigms. This access-mode distinction is central to understanding the design space of LLM-based steganography, as explored in Research Question 1 (Section~\ref{subsec:rq1}), which reveals a shift from white-box methods (11 studies) to black-box methods (11 studies) and hybrid approaches (5 studies) in recent literature, reflecting the field's evolution toward practical deployment.

However, LLMs \cite{shanahan2024talking} introduce new challenges. Their tendency toward \textbf{hallucinations} can create detectable artifacts, and the \textbf{Psic Effect} remains a fundamental constraint. Additionally, \textbf{segmentation ambiguity} introduced by subword-based language models presents a critical issue for provably secure linguistic steganography: when a sender detokenizes generated subword sequences into continuous text, the receiver might retokenize it differently, leading to decoding errors \cite{qi2024provably}. These challenges and their trade-offs are systematically analyzed in Research Question 5 (Section~\ref{subsec:rq5}).
