\section{Conclusion}

This systematic literature review has illuminated the profound impact of Large Language Models (LLMs) on linguistic steganography, demonstrating a clear paradigm shift toward context-aware, generative systems that prioritize imperceptibility, embedding capacity, and naturalness. By analyzing 18 primary studies (with 14 additional pending for full inclusion), we addressed key research questions, revealing that the published literature is rapidly evolving, with applications spanning secure communication in social media, zero-shot generation, and watermarking overlaps. Evaluation metrics such as Perplexity (PPL), Kullback-Leibler Divergence (KLD), and bits per token/word consistently show LLM-based methods outperforming traditional approaches, particularly through integration of external semantic resources like context retrieval and domain-specific prompts to enhance relevance and capacity. However, persistent limitations, including the Perceptual-Statistical Imperceptibility Conflict (Psic Effect), low entropy in short texts, and challenges in black-box access, underscore trade-offs in security and practicality.

Our findings establish that contextual compatibility—leveraging domain correlations and communicative patterns—is essential for robust steganographic systems, paving the way for more sophisticated covert channels resistant to both human and automated detection. These advancements hold significant implications for information security, enabling high-capacity hidden messaging in everyday digital interactions while mitigating risks like hallucinations and biases in LLMs. Looking ahead, future research should focus on mitigating segmentation ambiguity, developing provably secure black-box frameworks, and exploring multimodal integrations (e.g., text with images) to further bridge identified gaps. Ultimately, this review underscores the potential of LLMs to redefine steganography as a cornerstone of secure, imperceptible communication in an increasingly surveilled digital landscape.
