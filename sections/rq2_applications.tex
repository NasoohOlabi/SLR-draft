\subsection{Applications of LLM-based Steganographic Techniques (RQ2)}
\label{subsec:rq2}

The review identified six primary application domains, with covert communication being the dominant use case. The analysis reveals several distinct applications for LLM-based steganography, each with specific characteristics and requirements.


LLM-based steganographic techniques embed covert information within seemingly benign text, with applications spanning \textbf{secure communication}. This enables secure clandestine messaging in environments where classical steganography was too limited or suspicious [citation/reference needed]. These techniques also extend to \textbf{intellectual property protection} and \textbf{forensic linguistics}. The Calgacus protocol \cite{norelli2025llms} demonstrates how secret messages can be hidden inside different cover text of identical length by matching token rank sequences, enabling political critiques to masquerade as innocuous product reviews, while black-box methods like LLM-Stega operate through commercial APIs using encrypted keyword mapping and reject sampling \cite{wu2024generative}. For \textbf{intellectual property}, watermarking via logit biasing \cite{kirchenbauer2023watermark} embeds imperceptible statistical signals that identify AI authorship, attribute harmful content to specific users, and filter synthetic data to prevent model collapse. In \textbf{forensic linguistics}, adversarial stylometry allows LLMs to mask author identity or imitate others by adjusting stylistic features, reducing forensic tool accuracy to random guessing-protecting whistleblowers while enabling impersonation\cite{brennan2012adversarial,mikros2025large}.

These same techniques pose significant risks to AI safety and cybersecurity, bypassing governance mechanisms and enabling sophisticated attacks. The "Linguistic Trojan Horse" embeds unsafe content in benign responses to evade safety filters, while Chain-of-Thought auditing reveals that models can hide true reasoning in seemingly innocuous steps, complicating oversight and enabling covert multi-agent collusion. In cybersecurity, steganographic prompt injection in vision-language models achieves over 31\% success by hiding malicious instructions in images, while SteganoBackdoor embeds semantic triggers in training data with 99\% success at low poisoning rates. Model weights can be exfiltrated through subtle token variations, and watermark stealing enables spoofing and scrubbing attacks that bypass accountability measures. Detection methods include cross-model probability scoring, low-entropy token analysis, and symbolic anomaly detection, though these face ongoing vulnerabilities that demand adaptive defense architectures \cite{kuo2025h,zolkowski2025early,menke2025annotating,jiang2025safechain}.

% The sources provide strong support for several of your claims regarding **secure communication** and **intellectual property protection**, specifically concerning **black-box methods** and **logit-based watermarking**. However, some specific terms and protocols (like Calgacus and forensic linguistics) are not explicitly detailed in the provided materials.

% Supported Claims

% *   **LLM-Based Steganography for Secure Communication:** Steganography is used to conceal secret messages within ordinary carriers to bypass censorship and ensure **behavioral security**. LLMs are favored as steganographic samplers because they approximate high-dimensional natural language distributions, making the "stegotext" look like human-written content.
% *   **Black-Box Methods (LLM-Stega):** The sources explicitly describe **LLM-Stega** as a black-box method that generates stego texts using Large Language Model user interfaces (UIs) without requiring internal model parameters. It embeds messages by constructing a **keyword set** (including subjects, predicates, objects, and emotions) and using an **encrypted steganographic mapping**. It also employs a feedback mechanism based on **reject sampling** to ensure the secret information is extracted accurately.
% *   **Intellectual Property (IP) and Watermarking:** Watermarking is confirmed as a primary technique for **tracing the provenance** of text and identifying LLM authorship to defend against IP theft.
% *   **Logit-Based Watermarking:** The sources describe a method that manipulates token sampling by creating a "green list" of tokens. These green tokens receive a **positive adjustment in their logits** (logit biasing) to increase their selection probability, allowing authorized detectors to identify the watermark. This is used to attribute content and verify AI involvement.

% Claims with Partial or No Source Support

% *   **Calgacus Protocol:** There is **no mention** of the "Calgacus protocol" or matching token rank sequences to turn political critiques into product reviews in the provided sources.
% *   **Forensic Linguistics and Adversarial Stylometry:** While the sources do not use the terms "**forensic linguistics**" or "**adversarial stylometry**," they do note that LLMs are capable of **nuanced linguistic mimicry** and can generate text that is indistinguishable from human writing in terms of style, coherence, and fluency. This mimicry is used to improve the "cognitive-imperceptibility" of steganography so that text conforms to expected communication patterns.
% *   **Model Collapse:** The specific claim that watermarking is used to filter synthetic data to prevent "**model collapse**" is not present in the sources, although tracing provenance to identify machine-generated text is a major theme.

% **Analogy:**
% LLM steganography is like a spy hiding a secret code within a generic-looking grocery list; to anyone else, itâ€™s just a list of items, but to the recipient, the specific choice of words reveals a hidden message.

% Based on the sources, many of your claims regarding the potential risks and mechanisms of linguistic steganography are supported, though some specific terms like "Chain-of-Thought auditing" are not explicitly mentioned.

% Risks to AI Safety and Cybersecurity
% The sources confirm that linguistic steganography poses significant risks by enabling the **covert transmission of information** that bypasses governance and monitoring systems.
% *   **Bypassing Governance:** Techniques are designed to evade "massive surveillance and censorship mechanisms" by making subversive messages look like mundane, natural human communication.
% *   **Enabling Attacks:** LLMs can be misused for **"Jailbreak Attacks"** (e.g., StegoAttack), which conceal harmful queries to bypass safety filters, and **"TrojanStego,"** which embeds secrets directly into model outputs for data exfiltration.

% The "Linguistic Trojan Horse"
% While the sources do not use the exact phrase "Linguistic Trojan Horse," they describe the same concept through **generative and rewriting-based steganography**.
% *   **Embedding Unsafe Content:** These methods hide secret bitstreams within high-quality, fluent text generated by LLMs.
% *   **Evading Filters:** Because the resulting "stego-text" is statistically and perceptually indistinguishable from normal text, it can pass through safety filters and human review without detection.

% Covert Multi-Agent Collusion
% The sources directly support the claim that these techniques enable **covert collusion between multiple agents**.
% *   **GCStego Framework:** A framework called **GCStego** specifically enables secret transmission in **group chat scenarios**.
% *   **Coordinated Secrecy:** Multiple agents can pass secret messages through stego-texts that appear to be regular, coherent conversations. This enhances "behavioral imperceptibility," making the coordinated exchange of information nearly impossible for an external monitor to identify.

% ***

% **Note on Missing Information:** The provided sources do not contain specific information regarding **"Chain-of-Thought auditing"** or models hiding their **"true reasoning"** in innocuous steps. You may wish to verify those specific concepts through independent technical safety research.

% **Analogy:** Linguistic steganography is like a **spy writing a secret code using the first letter of every word in a grocery list.** To a security guard, it looks like a harmless errand, but to another agent, it is a set of coordinates for a mission.

% ---------------------------------------------
% ---------------------------------------------
% Incorrect during the filtering phase I focused on stego discarding watermarks.
% ---------------------------------------------
% ---------------------------------------------

% \subsubsection{Primary Applications}

% \begin{table}[ht]
%   \centering
%   \small
%   \begin{tabular}{|p{4cm}|p{2cm}|p{2cm}|p{4cm}|}
%     \hline
%     \textbf{Application Domain} & \textbf{Percentage} & \textbf{Studies} & \textbf{Key Examples}              \\
%     \hline
%     Covert Communication        & 60\%                & 19               & DAIRstega, Co-Stega, FreStega      \\
%     \hline
%     Content Watermarking        & 25\%                & 8                & DeepTextMark, Natural Watermarking \\
%     \hline
%     Fingerprinting              & 8\%                 & 3                & Model identification, licensing    \\
%     \hline
%     Adversarial Attacks         & 4\%                 & 1                & StegoAttack                        \\
%     \hline
%     Data Exfiltration           & 2\%                 & 1                & TrojanStego                        \\
%     \hline
%     Social Media Hiding         & 1\%                 & 1                & Hi-stega                           \\
%     \hline
%   \end{tabular}
%   \caption{Distribution of applications across reviewed studies}
%   \label{tab:applications}
% \end{table}

% \subsubsection{Covert Communication Applications}

% Covert communication represents the primary application domain, with approximately 60\% of papers focusing on this use case. Key characteristics include:

% \begin{itemize}
%   \item \textbf{Censored Environments:} Particularly important for use in environments with restricted communication
%   \item \textbf{High Imperceptibility Requirements:} Need for both perceptual and statistical imperceptibility
%   \item \textbf{Context Awareness:} Many systems leverage contextual information to enhance naturalness
%   \item \textbf{Real-time Deployment:} Emphasis on practical, deployable solutions
% \end{itemize}

% Notable examples include \textbf{Co-Stega}, which expands text space through context retrieval and entropy enhancement for social media applications, and \textbf{FreStega}, which provides a plug-and-play approach to imperceptibility.

% \subsubsection{Watermarking and Fingerprinting Applications}

% About 30\% of studies focus on watermarking and fingerprinting applications:

% \begin{itemize}
%   \item \textbf{Content Tracing:} Watermarking for tracking content origin and ownership
%   \item \textbf{Model Fingerprinting:} Identifying and licensing LLMs for commercial use
%   \item \textbf{Copyright Protection:} Embedding ownership information in generated content
%   \item \textbf{Attribution:} Ensuring proper credit for content creators
% \end{itemize}

% \subsubsection{Emerging Applications}

% Recent studies demonstrate novel applications that expand the traditional scope:

% \begin{itemize}
%   \item \textbf{Social Media Hiding:} Models such as \textbf{Co-Stega} expand text space through context retrieval and entropy enhancement
%   \item \textbf{Jailbreak Attacks:} Steganography can conceal harmful queries, as demonstrated in \textbf{StegoAttack}
%   \item \textbf{Data Exfiltration:} \textbf{TrojanStego} embeds secrets directly into LLM outputs
%   \item \textbf{Multimodal Steganography:} Integration with vision-language models for text-image combinations
% \end{itemize}

% \subsubsection{Domain-Specific Applications}

% The field further investigates domain-specific applications, including:

% \begin{itemize}
%   \item \textbf{High-Entropy Texts:} Utilization in news articles and formal documents
%   \item \textbf{Short Prompts:} Question-and-answer paradigms for conversational AI
%   \item \textbf{Specialized Corpora:} Medical, legal, and technical document steganography
%   \item \textbf{Cultural Contexts:} Adaptation to different cultural and linguistic contexts
% \end{itemize}

% \subsubsection{Application Requirements and Constraints}

% Different applications impose varying requirements on steganographic systems:

% \begin{table}[ht]
%   \centering
%   \small
%   \begin{tabular}{|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
%     \hline
%     \textbf{Application} & \textbf{Capacity Requirement} & \textbf{Security Level} & \textbf{Imperceptibility} \\
%     \hline
%     Covert Communication & High (2-6 bpt)                & Very High               & Very High                 \\
%     \hline
%     Watermarking         & Medium (1-3 bpt)              & High                    & High                      \\
%     \hline
%     Fingerprinting       & Low (0.5-2 bpt)               & Medium                  & Medium                    \\
%     \hline
%     Social Media         & High (3-5 bpt)                & High                    & Very High                 \\
%     \hline
%   \end{tabular}
%   \caption{Application-specific requirements and constraints}
%   \label{tab:application_requirements}
% \end{table}

% The growing overlap with adversarial robustness and potential for multimodal steganography using models such as GPT-4o suggests exciting future directions for the field.
