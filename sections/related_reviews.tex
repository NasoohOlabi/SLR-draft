\section{Related Reviews}

Majeed et al. (2021) \cite{math9212829} surveyed pre-LLM text steganography techniques, predating the current transformer era. Setiadi et al. (2025) \cite{Setiadi_Ghosal_Sahu_2025} recognizes that LLMs have "revitalized" linguistic steganography, examining recent methods (2021-2025) using GPT-2 \cite{radford2019gpt2}, GPT-3 \cite{brown2020languagemodelsfewshotlearners}, LLaMA2 \cite{touvron2023llama2openfoundation}, and Baichuan2 \cite{xiao2024baichuan2suminstructionfinetunebaichuan27b}. However, their review remains a critical examination rather than a systematic survey, leaving several key papers unaddressed. Crucially, the field has evolved from "statistical vector embedding" (Word2Vec, GloVe) to "language-model vector embedding" that exploits BERT-scale transformers and higher-dimensional semantic spaces.

This creates a methodological gap: no systematic review comprehensively maps how large-scale transformers have redefined text steganography. Modern advances extend beyond naive generation to sophisticated Controllable Text Generation (CTG) frameworks \cite{zhang2020linguistic}. These employ Variational Autoencoders (VAEs) to model latent features and Diffusion Models to inject randomness, mitigating spurious associations between secrets and control conditions. Classical surveys emphasized synonym replacement, spacing manipulation, and Huffman coding \cite{math9212829}—techniques that predated LLMs. Earlier methods relied on context-free grammars (CFGs) or Markov chains, often producing syntactically correct but semantically incoherent cover texts. Contemporary approaches leverage prompt learning and prefix tuning, enabling efficient model customization without costly full fine-tuning.

Defensive strategies must evolve accordingly. Traditional steganalysis, premised on hand-crafted statistical features, falters against generative steganography's high statistical concealment. Current research must confront "stegomalware"—attacks that conceal command-and-control communications within innocuous digital media.
