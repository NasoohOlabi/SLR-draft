\section{Related Reviews}

Previous reviews on text steganography, such as the one by Majeed et al. (2021) \cite{math9212829}, primarily focus on older techniques and were published before the widespread adoption of Large Language Model (LLM)-based approaches. While the more recent review by Setiadi et al. (2025) \cite{Setiadi_Ghosal_Sahu_2025} acknowledges that the field of linguistic steganography "has been revitalized by large language models (LLMs)" and specifically examines recent AI-powered steganography methods from the last three years (post-2021), detailing techniques that utilize models like GPT-2 \cite{radford2019gpt2}, GPT-3 \cite{brown2020languagemodelsfewshotlearners}, LLaMA2 \cite{touvron2023llama2openfoundation}, and Baichuan2 \cite{xiao2024baichuan2suminstructionfinetunebaichuan27b}, it is important to note that the Setiadi et al. (2025) review is not a systematic literature review. It's a "concise and critical examination" rather than an exhaustive survey, it does not include all relevant papers published between 2021 and 2025.

Consequently, despite the advancements discussed, a notable gap persists for a comprehensive systematic literature review that fully summarizes how large-scale transformers have reshaped text steganography. This is in contrast to earlier surveys that predominantly identified classical approaches such as synonym replacement, spacing, and Huffman coding, which predated the LLM revolution \cite{math9212829}.