\subsection{Evaluation Metrics and Methods (RQ3)}
\label{subsec:rq3}

Performance evaluation for LLM-based steganography relies on three key categories of metrics, with significant variation in reporting standards across studies. The analysis reveals both the diversity of evaluation approaches and the need for standardization.

% \subsubsection{Metric Categories and Standards}

\begin{table}[ht]
  \centering
  \small
  \begin{tabular}{|p{3cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|}
    \hline
    \textbf{Metric Type} & \textbf{Imperceptibility} & \textbf{Capacity} & \textbf{Security}  & \textbf{Usage} \\
    \hline
    Perceptual           & PPL: 3-300                & BPW: 0.5-6.0      & Detection: 50-98\% & 85\%           \\
    \hline
    Statistical          & KLD: 0-3.3                & BPT: 1.0-5.8      & F1: 0.5-0.99       & 70\%           \\
    \hline
    Semantic             & BLEU: 0.3-0.9             & ER: 0.2-0.4       & Acc: 0.5-0.99      & 60\%           \\
    \hline
    Human Eval           & MAUVE: 0.2-0.9            & -                 & -                  & 25\%           \\
    \hline
  \end{tabular}
  \caption{Evaluation metrics usage and typical ranges across studies}
  \label{tab:evaluation_metrics}
\end{table}

\subsubsection{Perplexity (PPL)}

An imperceptibility metric that measures fluency, with lower values indicating better naturalness. It is recognized as a sensitive and unreliable metric for language model evaluation due to several intrinsic limitations. First, it suffers from a "confidently wrong" problem: as Baeldung, et al. \cite{wang2022perplexity} notes, perplexity measures only internal consistency, allowing models to assign low perplexity to grammatically perfect but factually absurd statements like "The cat is on the ceiling," since it cannot assess truth or logic. Second, it exhibits a short-text bias  as Fang, et al. \cite{fang2024wrong} demonstrated that perplexity scores are artificially inflated for short sequences despite potentially higher fluency, making it an "unqualified referee" for fair evaluation. Third, comparability across models is impossible without identical tokenization, vocabulary size directly scales perplexity - a model with fewer tokens appears deceptively better \cite{morgan2024perplexity}. Fourth, perplexity fails to capture long-range dependencies in modern LLMs; Fang, et al. \cite{fang2024wrong} argue that averaging log-likelihood across all tokens obscures performance on crucial "key tokens" by favoring predictable filler words. Finally, the metric is easily gamed through repetition, Wang, et al. \cite{wang2022perplexity} finds that "perplexity cannot distinguish between right emphasis and abnormal repetition," rewarding redundant text with artificially low scores. These flaws-sensitivity to length, architectural incompatibility, semantic blindness, and exploitability-collectively render perplexity an inadequate benchmark for steganographic text quality assessment.


\subsubsection{MAUVE}
Another imperceptibility metric that Evaluates distributional similarity between generated and reference text by quantifying the gap between neural and human-authored text using divergence frontiers.While MAUVE provides a theoretically elegant way to measure distributional gaps between generated and reference text, it remains curiously underused-appearing in just 3 of 26 reviewed sources. The deeper issue is that reported scores are \textit{not directly comparable} across studies.

Scaling conventions alone create immediate confusion: CPG-LS reports on a 0.0-1.0 scale (achieving 0.9412) while other work uses 0-100 (with advanced white-box LLM samplers reaching 80-92). Hi-Stega's scores (0.1341-0.2051) look low by comparison, but actually represent nearly 10× improvement over its own baseline (0.0135)-demonstrating that absolute values only matter within their own context.

Architectural differences further complicate matters: CPG-LS employs BERT-based lexical substitution whereas Hi-Stega uses generative GPT-2 models, making cross-study rankings invalid without careful normalization. Dataset choice compounds the problem-CPG-LS evaluated on CC-100 while Hi-Stega used Yahoo! News comments.

Like comparing temperatures without knowing Celsius from Fahrenheit, a "30" only makes sense in its original context. Consequently, MAUVE scores work best as \textit{internal benchmarks} for comparing variants within a single study, not as universal performance indicators across different steganographic frameworks.

\subsubsection{Statistical Metrics}
Kullback-Leibler Divergence (KLD) and Jensen-Shannon Divergence (JSD) are information-theoretic metrics used to evaluate steganographic security. KLD quantifies information loss by measuring the relative entropy between cover and stego distributions, serving as the theoretical standard for security modeling despite being asymmetric and failing as a strict distance measure. JSD improves upon this as a symmetric, bounded variant that measures how far each distribution lies from their average, providing a more stable basis for formulating statistical imperceptibility bounds-particularly when language models approximate human text distributions. Together, these two attempt to capture how closely steganographic outputs mimic legitimate communication channels.

However, real-world application reveals critical reliability failures, most notably the Perceptual-Statistical Imperceptibility Conflict (Psic Effect). KLD and JSD scores increasingly diverge from human judgment as statistical optimization progresses: methods achieving superior divergence metrics often produce chaotic, low-quality text easily detected by human observers. This discrepancy manifests acutely in dataset dependency-identical methods yield KLDs of 19.507 on IMDB versus 8.295 on Twitter at equivalent embedding rates, rendering cross-paper comparisons meaningless. Further compounding this, researchers employ incompatible formulas (some using latent BERT features versus direct word distributions), feature spaces, and measurement scales, evidenced by Meteor's KLD ranging from 0.045 in one study to 7.491-11.845 in others. Consequently, these metrics function like rulers measuring paintings: they confirm technical dimensional accuracy while completely missing perceptual naturalness, necessitating parallel evaluation with human-centric measures to achieve genuine security.


% \subsubsection{Cognitive Metrics}


% \begin{itemize}
%   \item \textbf{BLEU Score:} Semantic similarity assessment
%   \item \textbf{BERTScore:} Contextual similarity using BERT embeddings
%   \item \textbf{SimCSE:} Sentence-level semantic similarity
% \end{itemize}


\subsubsection{Capacity Metrics}

Capacity is judged by four metrics:

\begin{itemize}
  \item \textbf{Bits per Token (BPT):}
        \begin{equation}
          \text{BPT} = \frac{\text{Total Secret Bits}}{\text{Total Tokens}}
        \end{equation}
  \item \textbf{Bits per Word (BPW):}
        \begin{equation}
          \text{BPW} = \frac{\text{Total Secret Bits}}{\text{Total Words}}
        \end{equation}
  \item \textbf{Embedding Rate (ER):} Average density of hidden information per textual unit
        \begin{equation}
          \text{ER} = \frac{1}{N} \sum_{i=1}^{N} \text{bits}_i
        \end{equation}
        where $N$ is the number of textual units (words, tokens, or sentences) and $\text{bits}_i$ is the number of bits embedded in the $i$-th unit.v
  \item \textbf{Utilization Rate:}
        \begin{equation}
          H = -\sum_{x \in \mathcal{X}} P(x) \log_2 P(x)
        \end{equation}
        \begin{equation}
          \text{UR} = \left( \frac{\text{Actual Bits Embedded}}{H} \right) \times 100\%
        \end{equation}
\end{itemize}

These quantities quantify how densely a secret is packed, yet they are riddled with systematic biases that invalidate cross-system comparison.

\begin{table}[ht]
  \centering
  \small
  \begin{tabular}{|p{2cm}|p{4.5cm}|p{5cm}|}
    \hline
    \textbf{Bias Category}       & \textbf{Core Problem}                                                           & \textbf{Critical Implication}                                                              \\
    \hline
    Tokenization Inconsistencies & Metrics depend entirely on specific tokenizers (e.g., GPT-2 BPE vs. word-level) & Direct comparisons across papers become meaningless when tokenization strategies differ    \\
    \hline
    The "Psic Effect"            & Conflicts between imperceptibility and statistical security are ignored         & High capacity may degrade human fluency while paradoxically improving detection resistance \\
    \hline
    Model Training Bias          & Utilization Rate calculations assume uniform token availability                 & Actual hiding space is smaller than theoretical entropy due to model frequency preferences \\
    \hline
    Reporting Ambiguities        & No standard definition of "capacity" across systems                             & Practice payload vs. effective payload distinctions create misleading efficiency claims    \\
    \hline
    Context Blindness            & Density metrics treat text as neutral bit containers                            & Semantic incoherence constitutes a security failure that BPW/BPT fails to penalize         \\
    \hline
  \end{tabular}
  \caption{Five primary bias categories affecting capacity metrics in steganographic evaluation}
  \label{tab:capacity_bias_categories}
\end{table}

Tokenization differences make “1 BPT” from one paper incomparable to “1 BPT” from another due to the use of different tokenizers.
The Psic effect shows that higher density can hurt human fluency yet help statistical evasion.
Model frequency preferences shrink the real alphabet to high-probability tokens, so naive entropy limits overstate usable space.
Ambiguous reporting—practice vs.\ effective payload, ER1 vs.\ ER2, Bit Length vs.\ Stego Length—lets authors cherry-pick flattering numbers.
Finally, BPW/BPT ignore semantics, rewarding gibberish that is obviously steganographic.

Recent works reveal additional distortions: loop-count overhead, dictionary-size caps, baseline-dependent “improvements,” watermarking goals that invert the desired signal, conversational filler that dilutes BPW, and nonlinear ER curves that make any single threshold misleading.

% In short, capacity metrics are necessary but blind: they tell us how heavy the truck is, not whether the cargo is broken, the units match, or the load fell off on the way.


\subsubsection{Security Metrics}

Security evaluation assesses resistance to detection and attacks:

\begin{itemize}
  \item \textbf{Detection Accuracy:} Performance of steganalysis classifiers
  \item \textbf{F1 Score:} Balanced precision-recall measure
  \item \textbf{Attack Resistance:} Performance degradation under various attacks
  \item \textbf{False Positive Rate:} Rate of incorrect detection
\end{itemize}



\begin{table}[ht]
  \centering
  \small
  \begin{tabular}{|p{2.5cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
    \hline
    \textbf{Method Type} & \textbf{Avg. PPL} & \textbf{Avg. KLD} & \textbf{Capacity} & \textbf{Security} & \textbf{Studies} \\
    \hline
    White-box            & 3-8               & 0-0.25            & 1.1-5.98 bpt      & 95-99\%           & 11               \\
    \hline
    Black-box            & 168-363           & 1.76-2.23         & 5.37 bpw          & 79-91\%           & 11               \\
    \hline
    Hybrid               & 50-150            & 0.5-1.5           & 2.0-4.0 bpt       & 90-95\%           & 5                \\
    \hline
    Watermarking         & 100-200           & 1.0-2.0           & 1.0-3.0 bpt       & 95-98\%           & 12               \\
    \hline
  \end{tabular}
  \caption{Performance comparison across method types}
  \label{tab:method_comparison}
\end{table}

\subsubsection{Evaluation Methods and Tools}

Evaluation methods encompass both automated tools and human assessment:

\begin{itemize}
  \item \textbf{Automated Tools:}
        \begin{itemize}
          \item Steganalysis classifiers (LS-CNN, BiLSTM-Dense, BERT-FT)
          \item Statistical analysis tools
          \item Semantic similarity measures
        \end{itemize}
  \item \textbf{Human Evaluation:}
        \begin{itemize}
          \item Fluency judgments
          \item Naturalness assessment
          \item Detection difficulty evaluation
        \end{itemize}
\end{itemize}

\subsubsection{Evaluation Challenges and Gaps}

Several significant challenges exist in current evaluation practices:

\begin{itemize}
  \item \textbf{Lack of Standardized Benchmarks:} Only 20\% of studies use common datasets, making comparison difficult
  \item \textbf{Inconsistent Reporting:} Different units, scales, and methodologies across studies
  \item \textbf{Limited Human Evaluation:} Only 25\% of studies include human assessment
  \item \textbf{Missing Robustness Testing:} 60\% of studies don't test against various attacks
  \item \textbf{Incomplete Evaluation:} Many studies focus on only one or two metric categories
\end{itemize}

\subsubsection{Recent Advances in Evaluation}

Recent studies have introduced more comprehensive evaluation approaches:

\begin{itemize}
  \item \textbf{Multi-metric Evaluation:} Combining perceptual, statistical, and semantic metrics
  \item \textbf{Attack-based Testing:} Systematic evaluation against various attack scenarios
  \item \textbf{Human-AI Collaborative Assessment:} Combining automated and human evaluation
  \item \textbf{Cross-domain Evaluation:} Testing across different text types and domains
\end{itemize}

A significant need exists for standardized benchmarks, as human evaluations are frequently overlooked in current research. Future work should prioritize the development of comprehensive evaluation frameworks that address these gaps.
