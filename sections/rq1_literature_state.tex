\subsection{State of Published Literature on LLM-based Steganography (RQ1)}
\label{subsec:rq1}

\subsubsection{Publication Trends and Distribution}
Our analysis reveals a significant surge in LLM-based steganography research since 2023, with approximately 17 new papers published in 2024–2025. The field has evolved from early white-box modifications to more practical hybrid and black-box approaches.


\begin{table}[ht]
  \centering
  \small
  \begin{tabular}{|p{3cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
    \hline
    \textbf{Year} & \textbf{2020} & \textbf{2021-2022} & \textbf{2023} & \textbf{2024-2025} & \textbf{Total} \\
    \hline
    Publications  & 2             & 3                  & 4             & 17                 & 26             \\
    \hline
  \end{tabular}
  \caption{Publication trends by year}
  \label{tab:publication_trends}
\end{table}


\begin{table}[h]
  \centering
  \small
  \begin{tabular}{p{4cm} p{9cm}}
    \hline
    \textbf{Model Type (\%)}     & \textbf{Models and Representative Works}  \\
    \hline
    Open-weight Models ($>$80\%) &
    GPT-2 \cite{ding2023discop,pang2024fremax,kaptchuk2021meteor,wang2023hi},
    LLaMA/LLaMA2 \cite{liao2024co,lin2024zero,qi2024provably,li2024semantic},
    BERT \cite{zheng2022general,zhang2024controllable,ding2023joint,ding2023context,ji2024principled,xiang2023cpg,zhang2020linguistic,qiang2023natural,yi2022alisa},
    OPT \cite{munyer2024deeptextmark},
    BART \cite{qiang2023natural,li2023rewriting},
    Qwen \cite{li2024imperceptible}                                          \\
    Proprietary Models (12\%)    &
    GPT-3.5/4, ChatGPT
    \cite{xu2024beyond,steinebach2024natural,wu2024generative,hao2025robust} \\
    Custom Architectures (8\%)   &
    From-scratch or task-specific models
    \cite{yang2020vae}                                                       \\
    \hline
  \end{tabular}
  \caption{Model usage across surveyed studies}
\end{table}


\begin{table}[h]
  \centering
  \small
  \begin{tabular}{p{3.5cm} p{9.5cm}}
    \hline
    \textbf{Region (\%)} & \textbf{Institutions (Representative Works)}                                                                                                                                                                                                                                                                     \\
    \hline
    Asia-Pacific (84\%)  &
    Primarily China-based institutions, notably Tsinghua University, University of Science and Technology of China, Beijing University of Posts and Telecommunications, Shanghai University, Yunnan University, and Zhongguancun Laboratory, with additional contributions from Nanyang Technological University (Singapore) and MM~’24 (Australia)
    \cite{yang2020vae,ding2023discop,li2023rewriting,ding2023context,wu2024generative,lin2024zero,ji2024principled,yi2022alisa,li2024semantic,xiang2023cpg,liao2024co,zhang2024controllable,pang2024fremax,zheng2022general,wang2023hi,li2024imperceptible,ding2023joint,zhang2020linguistic,qiang2023natural,qi2024provably,hao2025robust} \\
    North America (12\%) &
    Boston University, Johns Hopkins University, Texas Tech University, University of Nebraska Omaha
    \cite{kaptchuk2021meteor,xu2024beyond,munyer2024deeptextmark}                                                                                                                                                                                                                                                                           \\
    Europe (4\%)         &
    Fraunhofer SIT|ATHENE, Germany
    \cite{steinebach2024natural}                                                                                                                                                                                                                                                                                                            \\
    \hline
  \end{tabular}
  \caption{Geographic distribution of the papers}
\end{table}

% \subsubsection{Publication Venues}
% \begin{description}[leftmargin=1cm, labelwidth=2cm]
%   \item[Preprint Servers (4\%)] Foundational and contemporary works released prior to peer review, such as arXiv \cite{lin2024zero}.
%   \item[Top-Tier Venues (29\%)] High-impact research across security conferences (ACM CCS \cite{kaptchuk2021meteor}, IEEE S\&P \cite{ding2023discop}), AI journals (Artificial Intelligence \cite{qiang2023natural}), signal processing journals (IEEE/ACM TASLP \cite{ding2023context}), and multimedia conferences (ACM MM \cite{ji2024principled,wu2024generative}).

%   \item[Specialized Venues (67\%)] Targeted research in signal processing and forensics, including IEEE Signal Processing Letters \cite{yi2022alisa,zhang2020linguistic,xiang2023cpg,li2024semantic}, IEEE Transactions on Information Forensics and Security \cite{yang2020vae}, ARES \cite{steinebach2024natural}, IH\&MMSec \cite{liao2024co}, ICONIP \cite{wang2023hi}, IEEE TCDS \cite{ding2023joint}, DASFAA \cite{li2023rewriting}, IEEE Access \cite{munyer2024deeptextmark}, MMSP \cite{zheng2022general}, IEEE TDSC \cite{qi2024provably}, ICASSP \cite{zhang2024controllable,pang2024fremax}, ICME \cite{li2024imperceptible}, IJCNN \cite{xu2024beyond}, and Frontiers of Computer Science \cite{hao2025robust}.
% \end{description}
% \subsubsection{Publication Venues}

\begin{table}[h]
  \centering
  \small
  \begin{tabular}{p{4cm} p{9cm}}
    \hline
    \textbf{Venue Category (\%)} & \textbf{Representative Venues and Works}                                                                                                                                     \\
    \hline
    Preprint Servers (4\%)       &
    arXiv
    \cite{lin2024zero}                                                                                                                                                                                          \\
    Top-Tier Venues (29\%)       &
    ACM CCS \cite{kaptchuk2021meteor}, IEEE S\&P \cite{ding2023discop}, Artificial Intelligence \cite{qiang2023natural}, IEEE/ACM TASLP \cite{ding2023context}, ACM MM \cite{ji2024principled,wu2024generative} \\
    Specialized Venues (67\%)    &
    IEEE Signal Processing Letters \cite{yi2022alisa,zhang2020linguistic,xiang2023cpg,li2024semantic},
    IEEE Transactions on Information Forensics and Security \cite{yang2020vae},
    ARES \cite{steinebach2024natural},
    IH\&MMSec \cite{liao2024co},
    ICONIP \cite{wang2023hi},
    IEEE TCDS \cite{ding2023joint},
    DASFAA \cite{li2023rewriting},
    IEEE Access \cite{munyer2024deeptextmark},
    MMSP \cite{zheng2022general},
    IEEE TDSC \cite{qi2024provably},
    ICASSP \cite{zhang2024controllable,pang2024fremax},
    ICME \cite{li2024imperceptible},
    IJCNN \cite{xu2024beyond},
    Frontiers of Computer Science \cite{hao2025robust}                                                                                                                                                          \\
    \hline
  \end{tabular}
  \caption{Distribution of publication venues}
\end{table}

% -------------------------------------
% -------------------------------------
% For Agents: keep this commented out for now.
% -------------------------------------
% -------------------------------------
% \subsubsection{Research Gaps and Opportunities}

% Several significant gaps were identified:
% \begin{itemize}
%   % \item Limited focus on non-English languages (only 8\% of studies) I only searched for English-language publications
%   \item Insufficient attention to ethical implications (10\% address ethical concerns)
%   \item Lack of standardized evaluation benchmarks \cite{munyer2024deeptextmark}
%   \item Limited real-world deployment studies \cite{kaptchuk2021meteor}
% \end{itemize}




% -------------------------------------
% -------------------------------------
% For Agents: keep this commented out for now.
% -------------------------------------
% -------------------------------------


% \subsubsection{Key Trends and Evolution}

% The field has undergone significant evolution with several notable trends:

% \begin{itemize}
%   \item \textbf{Paradigm Shift:} Early works (pre-2024) primarily concentrated on white-box modifications, such as token sampling in GPT-2 \cite{ding2023discop,kaptchuk2021meteor,wang2023hi}, whereas recent trends demonstrate a shift toward hybrid and black-box approaches for more practical, real-world deployment
%   \item \textbf{Model Democratization:} The increasing availability of open-source LLMs has democratized research in this field, as evidenced by the widespread adoption of models like GPT-2 \cite{ding2023discop,kaptchuk2021meteor,wang2023hi}, LLaMA/LLaMA2 \cite{liao2024co,lin2024zero,qi2024provably}, and BERT \cite{zheng2022general,ding2023joint,ding2023context,ji2024principled,zhang2020linguistic,qiang2023natural,yi2022alisa}
%   \item \textbf{Integration with Watermarking:} Approximately 40\% of research integrates concepts from digital watermarking, creating hybrid approaches
%   \item \textbf{Context Awareness:} Growing emphasis on context-aware steganographic systems that leverage domain-specific knowledge
% \end{itemize}

% Recent examples include \textbf{FREmax} (2024), which advanced frequency-based sampling for improved imperceptibility, and \textbf{Hi-stega} (2024), which provides a hierarchical retrieval-augmented approach combining high payload with semantic coherence. These developments represent the cutting edge of the field and demonstrate the rapid pace of innovation.
