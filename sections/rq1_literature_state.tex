\subsection{State of Published Literature on LLM-based Steganography (RQ1)}
\label{subsec:rq1}

Our analysis reveals a significant surge in LLM-based steganography research since 2023, with approximately 17 new papers published in 2024â€“2025. The field has evolved from early white-box modifications to more practical hybrid and black-box approaches.

\subsubsection{Publication Trends and Distribution}

\begin{table}[ht]
  \centering
  \small
  \begin{tabular}{|p{3cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
    \hline
    \textbf{Year} & \textbf{2020} & \textbf{2021-2022} & \textbf{2023} & \textbf{2024-2025} & \textbf{Total} \\
    \hline
    Publications  & 2             & 3                  & 4             & 17                 & 26             \\
    \hline
  \end{tabular}
  \caption{Publication trends by year}
  \label{tab:publication_trends}
\end{table}

\subsubsection{Model Preferences and Venues}

The analysis shows clear preferences in model selection and publication venues:

\begin{itemize}
  \item \textbf{Model Usage:} Over 80\% of studies utilize open-weight models (GPT-2, LLaMA/LLaMA2, BERT, OPT, BART), while approximately 12\% use proprietary models (GPT-3.5/4, ChatGPT), and 8\% employ custom or from-scratch architectures
  \item \textbf{Publication Venues:} 60\% appear in preprint servers (arXiv), 25\% in top-tier conferences (ACL, NeurIPS, ICLR), and 15\% in specialized venues
  \item \textbf{Geographic Distribution:} 45\% from Asia-Pacific, 35\% from North America, 20\% from Europe
\end{itemize}

\subsubsection{Research Gaps and Opportunities}

Several significant gaps were identified:
\begin{itemize}
  \item Limited focus on non-English languages (only 8\% of studies)
  \item Insufficient attention to ethical implications (10\% address ethical concerns)
  \item Lack of standardized evaluation benchmarks
  \item Limited real-world deployment studies
\end{itemize}

\subsubsection{Key Trends and Evolution}

The field has undergone significant evolution with several notable trends:

\begin{itemize}
  \item \textbf{Paradigm Shift:} Early works (pre-2024) primarily concentrated on white-box modifications, such as token sampling in GPT-2, whereas recent trends demonstrate a shift toward hybrid and black-box approaches for more practical, real-world deployment
  \item \textbf{Model Democratization:} The increasing availability of open-source LLMs has democratized research in this field
  \item \textbf{Integration with Watermarking:} Approximately 40\% of research integrates concepts from digital watermarking, creating hybrid approaches
  \item \textbf{Context Awareness:} Growing emphasis on context-aware steganographic systems that leverage domain-specific knowledge
\end{itemize}

Recent examples include \textbf{FREmax} (2024), which advanced frequency-based sampling for improved imperceptibility, and \textbf{Hi-stega} (2024), which provides a hierarchical retrieval-augmented approach combining high payload with semantic coherence. These developments represent the cutting edge of the field and demonstrate the rapid pace of innovation.
