\section{Results and Discussion}
\label{sec:results_discussion}

\input{sections/generated_tables.tex}

% Organizing by research questions
\subsection{State of Published Literature on LLM-based Steganography}
\label{subsec:rq1}

This section summarizes the main findings from the systematic literature review, focusing on the characteristics and performance of various LLM-based linguistic steganography and watermarking models.

Large Language Models (LLMs) have emerged as a significant development in the field of natural language processing, profoundly impacting text generation and related applications like steganography and watermarking. Our review identified several key LLM-based steganography models, each with unique approaches, strengths, and performance metrics.

LLMs are \textbf{generative models} that can \textbf{approximate complex distributions like text-based communication}. They represent the best-known technique for this task. These models operate by taking context and parameters to output an explicit probability distribution over the next token (e.g., a character or a word). The next token is typically sampled randomly from this distribution, and the process repeats to generate output of a desired length.

The \textbf{quality of content generated by generative models is impressive} and continues to improve. This has led to LLMs blurring the boundary of high-quality text generation between humans and machines.

\subsection{Applications of LLM-based Steganographic Techniques}
\label{subsec:rq2}

LLMs are considered \textbf{favorable for generative text steganography} due to their ability to generate high-quality text. Researchers propose using generative models as steganographic samplers to embed messages into realistic communication distributions, such as text. This approach marks a departure from prior steganographic work, motivated by the public availability of high-quality models and significant efficiency gains.

LLMs like \textbf{GPT-2, LLaMA, and Baichuan2} are commonly used as basic generative models for steganography. Existing methods often utilize a language model and steganographic mapping, where secret messages are embedded by establishing a mapping between binary bits and the sampling probability of words within the training vocabulary.

New approaches, such as \textbf{LLM-Stega}, explore \textbf{black-box generative text steganography using the user interfaces (UIs) of LLMs}, thereby circumventing the requirement to access internal sampling distributions. This method constructs a keyword set and employs an encrypted steganographic mapping for embedding, proposing an optimization mechanism based on reject sampling for accurate extraction and rich semantics.

Another framework, \textbf{Co-Stega}, leverages LLMs to address the challenge of low capacity in social media by expanding the text space for hiding messages (through context retrieval) and \textbf{increasing the generated text's entropy via specific prompts} to enhance embedding capacity. This approach also aims to maintain text quality, fluency, and relevance.

The concept of \textbf{zero-shot linguistic steganography} with LLMs utilizes in-context learning, where samples of covertext are used as context to generate more intelligible stegotext using a question-answer (QA) paradigm.

LLMs are also employed in approaches like \textbf{ALiSa}, which directly conceals token-level secret messages in seemingly natural steganographic text generated by off-the-shelf BERT models equipped with Gibbs sampling.

\subsection{Evaluation Metrics and Methods for LLM-based Steganography}
\label{subsec:rq3}

\subsubsection{Imperceptibility Metrics}
Perceptual metrics include PPL (Perplexity), Distinct-n, MAUVE, and human evaluation. Statistical metrics include KLD (Kullback-Leibler Divergence), JSD (Jensen-Shannon Divergence), anti-steganalysis accuracy, and semantic similarity.

\subsubsection{Embedding Capacity Metrics}
Metrics include bits per token/word and embedding rate.

\subsection{Integration of External Knowledge Sources}
\label{subsec:rq4}

The increasing popularity of deep generative models has made it feasible for provably secure steganography to be applied in real-world scenarios, as they fulfill requirements for perfect samplers and explicit data distributions.

Some approaches leverage LLMs to address the challenge of low capacity in social media by expanding the text space for hiding messages through context retrieval. This integration of external knowledge enhances both the capacity and contextual relevance of the steganographic techniques.

\subsection{Limitations and Trade-offs in Current LLM-based Steganography}
\label{subsec:rq5}

\subsubsection{Perceptual vs. Statistical Imperceptibility (Psic Effect)}
Improving perceptual quality can reduce statistical security, and vice versa. This fundamental trade-off presents a significant challenge in the field.

\subsubsection{Low Embedding Capacity}
Short texts and strict semantics limit the amount of information that can be hidden. This is a particular challenge in applications where the cover text must appear natural and contextually appropriate.

\subsubsection{Lack of Semantic Control and Contextual Consistency}
Ensuring generated text matches intended meaning and context is difficult. LLMs may introduce unpredictability, bias, or leak information.

\subsubsection{Segmentation Ambiguity}
Subword tokenization in LLMs can create ambiguity in message extraction, as the same text can be tokenized differently depending on context.

\subsubsection{White-box vs. Black-box Access}
Traditional "white-box" methods necessitate sharing the exact language model and training vocabulary, which limits fluency, logic, and diversity compared to natural texts generated by LLMs. These methods also inevitably alter the sampling probability distribution, thereby posing security risks.

\subsubsection{Other Challenges}
Additional challenges include computational overhead, data integrity/reversibility issues, and ethical concerns such as biases, discrimination, and potential for generating insulting content. There is also a lack of provable security and rigor in many NLP steganography works.

\subsection{Future Research Directions}
\label{subsec:rq6}

Based on the identified gaps and challenges, several promising future research directions emerge:

\begin{itemize}
    \item \textbf{Improved Balance Between Perceptual and Statistical Imperceptibility}: Developing techniques that can maintain both high perceptual quality and statistical security.
    
    \item \textbf{Enhanced Embedding Capacity}: Exploring methods to increase the amount of information that can be hidden without compromising imperceptibility.
    
    \item \textbf{Better Semantic Control}: Advancing approaches that ensure generated steganographic text maintains intended meaning and contextual consistency.
    
    \item \textbf{Addressing Segmentation Ambiguity}: Developing robust techniques to handle the challenges posed by subword tokenization in LLMs.
    
    \item \textbf{Ethical Frameworks}: Establishing guidelines and frameworks for the ethical use of LLM-based steganography to prevent misuse.
    
    \item \textbf{Provable Security}: Advancing the theoretical foundations of LLM-based steganography to provide stronger security guarantees.
    
    \item \textbf{Efficient Computation}: Reducing the computational overhead associated with LLM-based steganography techniques.
\end{itemize}

The field of LLM-based steganography is rapidly evolving, with new models and techniques being developed to address these challenges and explore new possibilities.