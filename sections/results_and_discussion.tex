\section{Results and Discussion}
\label{sec:results_discussion}

\input{sections/generated_tables.tex}

% Organizing by research questions
\subsection{State of Published Literature on LLM-based Steganography}
\label{subsec:rq1}

This section summarizes the main findings from the systematic literature review, focusing on the characteristics and performance of various LLM-based linguistic steganography and watermarking models.

Our review identified several key LLM-based steganography models, each with unique approaches, strengths, and performance metrics. The analysis reveals how these models leverage the text generation capabilities of LLMs for covert communication purposes.

\subsection{Applications of LLM-based Steganographic Techniques}
\label{subsec:rq2}

Our analysis reveals several distinct approaches to LLM-based steganography:

\begin{itemize}
    \item \textbf{LLM-Stega} \cite{wu2024generative}: Black-box approach using LLM user interfaces without requiring access to internal sampling distributions.
    \item \textbf{Co-Stega}: Addresses low capacity in social media by expanding text space through context retrieval and entropy enhancement.
    \item \textbf{Zero-shot linguistic steganography}: Utilizes in-context learning with question-answer paradigms.
    \item \textbf{ALiSa}: Conceals token-level messages in BERT-generated text using Gibbs sampling.
\end{itemize}

\subsection{Evaluation Metrics and Methods for LLM-based Steganography}
\label{subsec:rq3}

\subsubsection{Imperceptibility Metrics}
Perceptual metrics include PPL (Perplexity), Distinct-n, MAUVE, and human evaluation. Statistical metrics include KLD (Kullback-Leibler Divergence), JSD (Jensen-Shannon Divergence), anti-steganalysis accuracy, and semantic similarity.

\subsubsection{Embedding Capacity Metrics}
Metrics include bits per token/word and embedding rate.

\subsection{Integration of External Knowledge Sources}
\label{subsec:rq4}

Deep generative models have enabled practical applications of provably secure steganography by fulfilling requirements for perfect samplers and explicit data distributions. Integration of external knowledge through context retrieval enhances both capacity and contextual relevance.

\subsection{Limitations and Trade-offs in Current LLM-based Steganography}
\label{subsec:rq5}

\subsubsection{Perceptual vs. Statistical Imperceptibility (Psic Effect)}
The Psic Effect (defined in Section~\ref{sec:terminology}) presents a significant challenge in balancing perceptual quality and statistical security.

\subsubsection{Low Embedding Capacity}
Short texts and strict semantics limit the amount of information that can be hidden. This is a particular challenge in applications where the cover text must appear natural and contextually appropriate.

\subsubsection{Lack of Semantic Control and Contextual Consistency}
Ensuring generated text matches intended meaning and context is difficult. LLMs may introduce unpredictability, bias, or leak information.

\subsubsection{Segmentation Ambiguity}
Subword tokenization in LLMs can create ambiguity in message extraction, as the same text can be tokenized differently depending on context.

\subsubsection{White-box vs. Black-box Access}
Traditional white-box methods require access to exact language models and training vocabularies, limiting naturalness and introducing security risks through altered probability distributions.

\subsubsection{Other Challenges}
Additional challenges include computational overhead, data integrity/reversibility issues, and ethical concerns such as biases, discrimination, and potential for generating insulting content. There is also a lack of provable security and rigor in many NLP steganography works.

\subsection{Future Research Directions}
\label{subsec:rq6}

Based on the identified gaps and challenges, several promising future research directions emerge:

\begin{itemize}
    \item \textbf{Improved Balance Between Perceptual and Statistical Imperceptibility}: Developing techniques that can maintain both high perceptual quality and statistical security.
    
    \item \textbf{Enhanced Embedding Capacity}: Exploring methods to increase the amount of information that can be hidden without compromising imperceptibility.
    
    \item \textbf{Better Semantic Control}: Advancing approaches that ensure generated steganographic text maintains intended meaning and contextual consistency.
    
    \item \textbf{Addressing Segmentation Ambiguity}: Developing robust techniques to handle the challenges posed by subword tokenization in LLMs.
    
    \item \textbf{Ethical Frameworks}: Establishing guidelines and frameworks for the ethical use of LLM-based steganography to prevent misuse.
    
    \item \textbf{Provable Security}: Advancing the theoretical foundations of LLM-based steganography to provide stronger security guarantees.
    
    \item \textbf{Efficient Computation}: Reducing the computational overhead associated with LLM-based steganography techniques.
\end{itemize}

The field of LLM-based steganography is rapidly evolving, with new models and techniques being developed to address these challenges and explore new possibilities.