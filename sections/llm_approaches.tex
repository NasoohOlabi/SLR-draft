\section{Steganography and Large Language Models}
\label{sec:llm_approaches}

This section establishes the design space for LLM-based linguistic steganography, organizing methods along key dimensions that will be used throughout this review. We first explain why LLMs are well-suited for steganography, then introduce the design space axes, position key methods within this space, and clarify how evaluation metrics map to the imperceptibility dimensions introduced in Section~\ref{sec:background}.

\subsection{LLMs as Approximators of Natural Communication}

Large Language Models (LLMs) are autoregressive, generative systems based on the Transformer architecture \cite{vaswani2017attention} that approximate high-dimensional distributions over natural-language sequences \cite{kaptchuk2021meteor}\cite{radford2019language}. Given a prefix, an LLM emits a probability vector over the vocabulary; the next token is sampled from this vector and appended to the prefix, and the process repeats until a stopping criterion is met. During pre-training, billions of parameters are tuned on large web corpora so that the model's predictive distribution converges to the empirical distribution of the data \cite{brown2020language}. As a consequence, modern LLMs routinely produce text whose fluency, coherence and style are indistinguishable from human writing \cite{bubeck2023sparks}. The learned latent representations capture stylistic and semantic regularities that generalize across domains, enabling applications requiring nuanced linguistic mimicry \cite{zhang2023language}.

This ability to approximate natural language distributions makes LLMs powerful tools for steganography. As discussed in Section~\ref{sec:background}, achieving high channel entropy and perfect sampling is crucial for secure steganography. LLMs, with their learned distributions over natural language, provide high-entropy channels that enable embedding rates approaching theoretical limits while maintaining imperceptibility across perceptual, statistical, and cognitive dimensions.

\subsection{Design Space for LLM-Based Steganography}

LLM-based steganographic methods can be organized along three primary axes that define the design space:

\subsubsection{Access Mode Axis}

The \textbf{access mode} determines how the method interacts with the LLM:
\begin{itemize}
    \item \textbf{White-box}: Direct access to model internals (vocabulary, probability distributions, parameters), enabling fine-grained control over sampling and supporting provable security guarantees. Examples include methods that modify token sampling probabilities in GPT-2 or LLaMA.
    \item \textbf{Black-box}: Access only through APIs or user interfaces, without internal model access. Methods must work with generated text outputs, often using reject sampling or prompt engineering. Examples include \textbf{LLM-Stega} \cite{wu2024generative} and \textbf{Natural Watermarking} \cite{steinebach2024natural}.
    \item \textbf{Hybrid}: Combines elements of both paradigms, such as using white-box access for training or fine-tuning but black-box for deployment.
\end{itemize}

\subsubsection{Generation Style Axis}

The \textbf{generation style} determines how steganographic text is produced:
\begin{itemize}
    \item \textbf{De novo generation}: The method generates steganographic text from scratch, embedding the secret message during generation. Examples include \textbf{DAIRstega} and interval-based sampling methods.
    \item \textbf{Rewriting}: The method takes existing cover text and rewrites it to embed the secret message while preserving meaning. Examples include \textbf{Rewriting-based methods} \cite{li2023rewriting}.
    \item \textbf{Watermarking/Fingerprinting}: The method embeds ownership or identification information rather than arbitrary secret messages. Examples include \textbf{DeepTextMark} and model fingerprinting approaches.
\end{itemize}

\subsubsection{Context Usage Axis}

The \textbf{context usage} determines how the method handles contextual compatibility (as defined in Section~\ref{sec:background}):
\begin{itemize}
    \item \textbf{Explicit context}: The method explicitly incorporates external context. Examples include \textbf{Co-Stega} \cite{liao2024co}, which uses context retrieval for social media applications, and \textbf{Hi-Stega} \cite{wang2023hi}, which leverages social media context.
    \item \textbf{Implicit context}: The method leverages context inherent in the model's training or generation process. Examples include methods that use in-context learning or few-shot prompting.
    \item \textbf{No context}: The method generates text without explicit consideration of communicative context. Examples include basic generative methods that sample from the model distribution without context constraints.
\end{itemize}

\subsection{Positioning Key Methods in the Design Space}

To illustrate how methods map to this design space, we position several representative approaches:

\begin{itemize}
    \item \textbf{LLM-Stega} \cite{wu2024generative}: Black-box, de novo generation, implicit context. Uses LLM user interfaces with keyword-based mapping and reject sampling.
    \item \textbf{Co-Stega} \cite{liao2024co}: Hybrid (can work with both), de novo generation, explicit context. Expands text space through context retrieval and increases entropy via prompts for social media applications.
    \item \textbf{Hi-Stega} \cite{wang2023hi}: White-box or hybrid, de novo generation, explicit context. Leverages social media context to maintain semantic and contextual consistency.
    \item \textbf{ALiSa} \cite{yi2022alisa}: White-box, de novo generation, implicit context. Uses BERT with Gibbs sampling for token-level embedding.
    \item \textbf{Zero-shot methods} \cite{lin2024zero}: Black-box, de novo generation, explicit context. Uses in-context learning with question-answer paradigms.
    \item \textbf{Provably secure methods} \cite{kaptchuk2021meteor, ding2023discop, qi2024provably}: White-box, de novo generation, typically no context or implicit context. Focus on mathematical security guarantees and perfect sampling.
\end{itemize}

This design space provides the framework for classifying and comparing studies in the systematic review, as presented in Section~\ref{sec:results}. The classification enables systematic analysis of how different design choices affect performance metrics, application suitability, and trade-offs.

\subsection{Evaluation Metrics and Imperceptibility Dimensions}

Evaluation metrics map directly to the three dimensions of imperceptibility introduced in Section~\ref{sec:background}:

\subsubsection{Perceptual Imperceptibility Metrics}

These metrics assess human naturalness and fluency:
\begin{itemize}
    \item \textbf{Perplexity (PPL)} \cite{holtzman2019curious}: Measures how well the model predicts the text; lower PPL indicates higher fluency. However, PPL values depend on the underlying language model used for evaluation (GPT-2, LLaMA, etc.) and text length, making cross-study comparisons challenging.
    \item \textbf{Distinct-n} \cite{li2016distinct}: Measures lexical diversity by counting unique n-grams.
    \item \textbf{MAUVE} \cite{pillutla2021mauve}: Measures distributional similarity between generated and reference text.
    \item \textbf{Human evaluation}: Direct assessment of naturalness, fluency, and coherence by human judges.
\end{itemize}

\subsubsection{Statistical Imperceptibility Metrics}

These metrics assess distributional similarity and resistance to steganalysis:
\begin{itemize}
    \item \textbf{Kullback-Leibler Divergence (KLD)}: Measures how much the steganographic text distribution differs from natural text. Lower KLD indicates better statistical imperceptibility, but measurements depend on the reference dataset used.
    \item \textbf{Jensen-Shannon Divergence (JSD)}: A symmetric variant of KLD.
    \item \textbf{Anti-steganalysis accuracy}: The accuracy of steganalysis models in detecting steganographic text; lower accuracy indicates better security. This is a critical metric for assessing practical security.
\end{itemize}

\subsubsection{Cognitive Imperceptibility Metrics}

These metrics assess semantic and contextual fidelity:
\begin{itemize}
    \item \textbf{Semantic similarity} \cite{papineni2002bleu}: Measures semantic preservation using metrics like BLEU, ROUGE, or embedding-based similarity.
    \item \textbf{Domain-specific evaluations}: Assessments of whether generated text is appropriate for its intended context (e.g., social media appropriateness, technical accuracy).
\end{itemize}

\subsubsection{Embedding Capacity Metrics}

These metrics quantify the amount of information that can be embedded:
\begin{itemize}
    \item \textbf{Bits per token (bpt)}: The number of secret bits embedded per generated token.
    \item \textbf{Bits per word (bpw)}: The number of secret bits embedded per word.
    \item \textbf{Embedding rate}: The ratio of embedded bits to total text length.
\end{itemize}

The inconsistent application of these metrics across studies (e.g., different reference models for PPL, different reference datasets for KLD, mixing bpt and bpw) creates challenges for cross-method comparison, as discussed in Section~\ref{sec:introduction} and systematically analyzed in Research Question 3 (Section~\ref{subsec:rq3}). The analysis reveals that while 85\% of studies report perceptual metrics, only 70\% report statistical metrics, and 60\% report cognitive metrics, with significant variation in how these metrics are calculated and reported.
