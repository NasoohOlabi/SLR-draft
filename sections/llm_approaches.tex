% \section{Steganography and Large Language Models}
% \label{sec:llm_approaches}

% Large Language Models (LLMs) have emerged as a significant development in the field of natural language processing, profoundly impacting text generation and related applications like steganography and watermarking. Here's a breakdown of their emergence and impact:

% \subsection{Capabilities and Approximating Natural Communication}
% LLMs are \textbf{generative models} that can \textbf{approximate complex distributions like text-based communication}\cite{kaptchuk2021meteor}. They represent the best-known technique for this task.
% These models operate by taking context and parameters to output an explicit probability distribution over the next token (e.g., a character or a word). The next token is typically sampled randomly from this distribution, and the process repeats to generate output of a desired length.

% Training LLMs involves processing vast amounts of data to set parameters and structure, enabling their output distributions to approximate true distributions in the training data.

% The \textbf{quality of content generated by generative models is impressive} and continues to improve. This has led to LLMs blurring the boundary of high-quality text generation between humans and machines.

% LLMs are increasingly used to generate data for specific tasks, such as tabular data, relational triples, sentence pairs, and instruction data, often achieving satisfactory generation quality in zero-shot learning for specific subject categories.
% They have also shown capabilities in mimicking language styles and semantics, and their generalization ability allows them to comprehend the semantics of context.

\section{Steganography and Large Language Models}
\label{sec:llm_approaches}


\subsection{Capabilities and Approximating Natural Communication}
Large Language Models (LLMs) are autoregressive, generative systems based on the Transformer architecture \cite{vaswani2017attention} that approximate high-dimensional distributions over natural-language sequences \cite{kaptchuk2021meteor}\cite{radford2019language}.
Given a prefix, an LLM emits a probability vector over the vocabulary; the next token is sampled from this vector and appended to the prefix, and the process repeats until a stopping criterion is met.
During pre-training, billions of parameters are tuned on large web corpora so that the modelâ€™s predictive distribution converges to the empirical distribution of the data \cite{brown2020language}.
As a consequence, modern LLMs routinely produce text whose fluency, coherence and style are indistinguishable from human writing \cite{bubeck2023sparks}. The learned latent representations capture stylistic and semantic regularities that generalize across domains, enabling applications requiring nuanced linguistic mimicry \cite{zhang2023language}.


\subsection{Role in Generative Linguistic Steganography}

LLMs are considered \textbf{favorable for generative text steganography} due to their ability to generate high-quality text.
Researchers propose using generative models as steganographic samplers to embed messages into realistic communication distributions, such as text. This approach marks a departure from prior steganographic work, motivated by the public availability of high-quality models and significant efficiency gains.

LLMs like \textbf{GPT-2} \cite{radford2019language}, \textbf{LLaMA} \cite{touvron2023llama}, and \textbf{Baichuan2} \cite{yang2023baichuan2} are commonly used as basic generative models for steganography.
Existing methods often utilize a language model and steganographic mapping, where secret messages are embedded by establishing a mapping between binary bits and the sampling probability of words within the training vocabulary.
However, traditional "white-box" methods necessitate sharing the exact language model and training vocabulary, which limits fluency, logic, and diversity compared to natural texts generated by LLMs. These methods also inevitably alter the sampling probability distribution, thereby posing security risks \cite{wu2024generative}.

New approaches, such as \textbf{LLM-Stega} \cite{wu2024generative}, explore \textbf{black-box generative text steganography using the user interfaces (UIs) of LLMs}. This circumvents the requirement to access internal sampling distributions. The method constructs a keyword set and employs an encrypted steganographic mapping for embedding. It proposes an optimization mechanism based on reject sampling for accurate extraction and rich semantics \cite{wu2024generative}.

Another framework, \textbf{Co-Stega}, leverages LLMs to address the challenge of low capacity in social media. It expands the text space for hiding messages through context retrieval and \textbf{increases the generated text's entropy via specific prompts} to enhance embedding capacity. This approach also aims to maintain text quality, fluency, and relevance \cite{liao2024co}.

The concept of \textbf{zero-shot linguistic steganography} with LLMs utilizes in-context learning, where samples of covertext are used as context to generate more intelligible stegotext using a question-answer (QA) paradigm \cite{lin2024zero}.
LLMs are also employed in approaches like \textbf{ALiSa}, which directly conceals token-level secret messages in seemingly natural steganographic text generated by off-the-shelf BERT \cite{devlin2018bert} models equipped with Gibbs sampling \cite{yi2022alisa}.

The increasing popularity of deep generative models has made it feasible for provably secure steganography to be applied in real-world scenarios, as they fulfill requirements for perfect samplers and explicit data distributions (see Section~\ref{sec:terminology}) \cite{ding2023discop, kaptchuk2021meteor, qi2024provably}.

\subsection{LLM-Based Steganography Models}

\subsubsection{Evaluation Metrics}

\paragraph{Imperceptibility Metrics}
Perceptual metrics include PPL \cite{holtzman2019curious}, Distinct-n \cite{li2016distinct}, MAUVE \cite{pillutla2021mauve}, and human evaluation. Statistical metrics include KLD, JSD, anti-steganalysis accuracy, and semantic similarity \cite{papineni2002bleu}.

\paragraph{Embedding Capacity Metrics}
Metrics include bits per token/word and embedding rate.

\subsection{Challenges and Limitations in Steganography with LLMs}

\subsubsection{Perceptual vs. Statistical Imperceptibility (Psic Effect)}
The \textbf{Psic Effect} \cite{yang2020vae} represents a fundamental trade-off in steganographic systems.

\subsubsection{Low Embedding Capacity}
Short texts and strict semantics limit the amount of information that can be hidden.

\subsubsection{Lack of Semantic Control and Contextual Consistency}
Ensuring generated text matches intended meaning and context is difficult.

\subsubsection{Challenges with LLMs in Steganography}
LLMs may introduce unpredictability, bias, or leak information.

\subsubsection{Segmentation Ambiguity}
Tokenization can cause ambiguity in how information is embedded or extracted.

A primary challenge in steganography, particularly when utilizing Large Language Models (LLMs), revolves around the \textbf{distinction between white-box and black-box access}. Most current advanced generative text steganographic methods operate under a "white-box" paradigm, meaning they require direct access to the LLM's internal components, such as its training vocabulary and the sampling probabilities of words. This presents a significant limitation because many state-of-the-art LLMs are proprietary and are accessed by users primarily through black-box APIs or user interfaces \cite{wu2024generative}. Consequently, these white-box methods are often impractical for real-world deployment with popular commercial LLMs. Furthermore, methods that rely on modifying the sampling probability distribution to embed secret messages inherently introduce security risks because they alter the original distribution, making the steganographic text statistically distinguishable from normal text \cite{yang2020vae, kaptchuk2021meteor, ding2023discop, wu2024generative}.

Another significant hurdle is \textbf{ensuring both the quality and imperceptibility of the generated text}, encompassing perceptual, statistical, and cognitive imperceptibility \cite{ding2023context}. While advancements in deep neural networks have improved text fluency and embedding capacity, older models or certain embedding strategies can still produce texts that lack naturalness, logical coherence, or diversity compared to human-written content. Linguistic steganography methods often struggle to control the semantics and contextual characteristics of the generated text, leading to a decline in its "cognitive-imperceptibility" \cite{yang2020vae, ding2023context}. This can make concealed messages easier for human or machine supervisors to detect.
Although models like NMT-Stega and Hi-Stega aim to maintain semantic and contextual consistency by leveraging source texts or social media contexts, this remains a complex challenge \cite{ding2023context, wang2023hi}.

\textbf{Channel entropy requirements and variability} also pose a considerable challenge. Traditional universal steganographic schemes often demand consistent channel entropy, which is rarely maintained in real-world natural language communication. Moments of low or zero entropy can cause protocols to fail or require extraordinarily long steganographic texts. The Psic Effect highlights this dilemma in balancing quality and detectability.

Furthermore, \textbf{segmentation ambiguity} introduced by subword-based language models presents a critical issue for provably secure linguistic steganography. When a sender detokenizes generated subword sequences into continuous text, the receiver might retokenize it differently, leading to decoding errors \cite{qi2024provably}.

Additional limitations include:
\begin{itemize}
    \item \textbf{Computational Overhead}: LLMs incur 3-5 times higher computational cost than prior methods \cite{lin2024zero}.
    \item \textbf{Data Integrity and Reversibility}: Some methods cannot perfectly recover the original cover text after message extraction \cite{zheng2022general, qiang2023natural}.
    \item \textbf{Ethical Concerns}: Pre-trained LLMs may introduce biases, discrimination, or inappropriate content \cite{lin2024zero, bender2021dangers}.
    \item \textbf{Provable Security}: Many NLP steganography works lack rigorous security analyses and fail to meet formal cryptographic definitions \cite{kaptchuk2021meteor}.
    \item \textbf{Hallucinations}: LLMs can generate factually incorrect or contextually inappropriate content, leading to embedding errors \cite{holtzman2019curious}.
    \item \textbf{Channel Entropy Limitations}: Short, context-dependent texts have lower entropy, limiting hiding capacity \cite{liao2024co}.
\end{itemize}
