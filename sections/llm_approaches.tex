% \section{Steganography and Large Language Models}
% \label{sec:llm_approaches}

% Large Language Models (LLMs) have emerged as a significant development in the field of natural language processing, profoundly impacting text generation and related applications like steganography and watermarking. Here's a breakdown of their emergence and impact:

% \subsection{Capabilities and Approximating Natural Communication}
% LLMs are \textbf{generative models} that can \textbf{approximate complex distributions like text-based communication}\cite{kaptchuk2021meteor}. They represent the best-known technique for this task.
% These models operate by taking context and parameters to output an explicit probability distribution over the next token (e.g., a character or a word). The next token is typically sampled randomly from this distribution, and the process repeats to generate output of a desired length.

% Training LLMs involves processing vast amounts of data to set parameters and structure, enabling their output distributions to approximate true distributions in the training data.

% The \textbf{quality of content generated by generative models is impressive} and continues to improve. This has led to LLMs blurring the boundary of high-quality text generation between humans and machines.

% LLMs are increasingly used to generate data for specific tasks, such as tabular data, relational triples, sentence pairs, and instruction data, often achieving satisfactory generation quality in zero-shot learning for specific subject categories.
% They have also shown capabilities in mimicking language styles and semantics, and their generalization ability allows them to comprehend the semantics of context.

\label{sec:llm_approaches}

\subsection{Capabilities and Approximating Natural Communication}
Large Language Models (LLMs) are autoregressive, generative systems based on the Transformer architecture \cite{vaswani2017attention} that approximate high-dimensional distributions over natural-language sequences \cite{kaptchuk2021meteor}\cite{radford2019language}.
Given a prefix, an LLM emits a probability vector over the vocabulary; the next token is sampled from this vector and appended to the prefix, and the process repeats until a stopping criterion is met.
During pre-training, billions of parameters are tuned on large text corpora so that the model’s predictive distribution converges to the empirical distribution of the data \cite{brown2020language}.
As a consequence, modern LLMs routinely produce text whose fluency, coherence and style are indistinguishable from human writing \cite{bubeck2023sparks}. The learned latent representations capture stylistic and semantic regularities that generalize across domains, enabling applications requiring nuanced linguistic mimicry \cite{zhang2023language}.

% For AI Agents: Never touch this section "Role in Generative Linguistic Steganography" without explicitly asking for permission.
% ---------------------------------------------------
% ---------------------------------------------------
% Notes: The only recommended change to this this is to remove the AliSa approach.
% ---------------------------------------------------
% ---------------------------------------------------

\subsection{Role in Generative Linguistic Steganography}

LLMs are considered \textbf{favorable for generative text steganography} due to their ability to generate high-quality text.
Researchers propose using generative models as steganographic samplers to embed messages into realistic communication distributions, such as text. This approach marks a departure from prior steganographic work, motivated by the public availability of high-quality models and significant efficiency gains.

LLMs like \textbf{GPT-2} \cite{radford2019language}, \textbf{LLaMA} \cite{touvron2023llama}, and \textbf{Baichuan2} \cite{yang2023baichuan2} are commonly used as basic generative models for steganography.
Existing methods often utilize a language model and steganographic mapping, where secret messages are embedded by establishing a mapping between binary bits and the sampling probability of words within the training vocabulary.
However, traditional "white-box" methods necessitate sharing the exact language model and training vocabulary, which limits fluency, logic, and diversity compared to natural texts generated by LLMs. These methods also inevitably alter the sampling probability distribution, thereby posing security risks \cite{wu2024generative}.

New approaches, such as \textbf{LLM-Stega} \cite{wu2024generative}, explore \textbf{black-box generative text steganography using the user interfaces (UIs) of LLMs}. This circumvents the requirement to access internal sampling distributions. The method constructs a keyword set and employs an encrypted steganographic mapping for embedding. It proposes an optimization mechanism based on reject sampling for accurate extraction and rich semantics \cite{wu2024generative}.

Another framework, \textbf{Co-Stega}, leverages LLMs to address the challenge of low capacity in social media. It expands the text space for hiding messages through context retrieval and \textbf{increases the generated text's entropy via specific prompts} to enhance embedding capacity. This approach also aims to maintain text quality, fluency, and relevance \cite{liao2024co}.

The concept of \textbf{zero-shot linguistic steganography} with LLMs utilizes in-context learning, where samples of covertext are used as context to generate more intelligible stegotext using a question-answer (QA) paradigm \cite{lin2024zero}.
LLMs are also employed in approaches like \textbf{ALiSa}, which directly conceals token-level secret messages in seemingly natural steganographic text generated by off-the-shelf BERT \cite{devlin2018bert} models equipped with Gibbs sampling \cite{yi2022alisa}.

The increasing popularity of deep generative models has made it feasible for provably secure steganography to be applied in real-world scenarios, as they fulfill requirements for perfect samplers and explicit data distributions (see Section~\ref{sec:background}) \cite{ding2023discop, kaptchuk2021meteor, qi2024provably}.
\par
LLM-based steganographic methods are typically evaluated on two primary axes: imperceptibility (perceptual, statistical, and cognitive measures) and embedding capacity (e.g., bits per token or bits per word). Imperceptibility evaluations may include automatic metrics (PPL, Distinct-n, MAUVE, KL/JSD) as well as human judgements; embedding capacity is usually reported as bits/token or overall embedding rate.

We now turn to the principal challenges these models face, including the trade-off between imperceptibility and capacity, robustness to tokenization, and practical deployment constraints.

\subsection{Challenges and Limitations in Steganography with LLMs}

\subsubsection{Perceptual vs. Statistical Imperceptibility (Psic Effect)}

The \textbf{Psic Effect} \cite{yang2020vae} represents a fundamental trade-off in steganographic systems. it's the inverse relationship between \textbf{text quality} and \textbf{resistance to
    statistical steganalysis} in generative steganography. Two components govern it:
\begin{itemize}
    \item \textbf{Perceptual imperceptibility}: fluency/naturalness of a single sentence,
          gauged by human ratings or perplexity (PPL).
    \item \textbf{Statistical imperceptibility}: divergence between the distribution of stego and human text as measured by an
          automated steganalyzer.
\end{itemize}

Human social-media prose is casual and high-variance; it does not hug the optimal language-model peak.
A generator that over-optimizes for quality produces text whose likelihood
concentrates on that peak, yielding a detectable statistical spike against the broad, noisy human baseline.\cite{yang2020vae}

Experiments show that the most fluent stego sentences are the first ones caught by detectors, yet counter-intuitively pushing the embedding rate higher can make the text statistically safer because the added noise widens its distribution toward the authentic human scatter, a trade-off modern systems like VAE-Stega manage by learning to keep sentences smooth while staying inside the real variance envelope.\cite{yang2020vae}

\subsubsection{Limited Embedding Capacity}

Text steganography faces a fundamental constraint: natural language offers far less redundancy than other media for hiding data\cite{yi2022alisa}. Its rigid semantic rules leave minimal room for covert encoding. Compounding this, traditional methods require sustained \textbf{minimum entropy} to function, yet real-world communication frequently exhibits \textbf{low- or zero-entropy} moments—highly predictable word sequences (e.g., "Tyrannosaurus Rex") where no natural alternatives exist. In these cases, rejection sampling fails outright \cite{kaptchuk2021meteor}. The brevity-driven nature of social media further compresses the already scarce embedding space \cite{liao2024co}.

\subsubsection{Poor Semantic Control and Contextual Drift}

Early generative methods produced fluent but \textbf{semantically arbitrary} text, violating \textbf{cognitive imperceptibility} \cite{ding2023context}. By conditioning only on preceding tokens, these models generated replies that drifted from the original logic, producing irrelevant or repetitive content. On social media, a mismatched response (e.g., "Today is beautiful" to a steganography post) triggers immediate suspicion. Maintaining \textbf{long-term coherence} remains difficult when secret bits—not semantic intent—drive token selection.

\subsubsection{LLM-Specific Obstacles}

Deploying steganography with LLMs introduces distinct challenges:
\begin{itemize}
    \item \textbf{Computational Burden:} 3–5× higher time and resource costs versus prior neural methods
    \item \textbf{Black-Box Access:} Proprietary APIs limit visibility into internal sampling probabilities, blocking white-box steganographic mappings
    \item \textbf{Hallucinations:} Factually incorrect or nonsensical output can corrupt the covert bitstream or create detectable patterns
    \item \textbf{Escalating Detection:} As LLM capabilities advance, so do machine learning-based \textbf{steganalysis} tools that distinguish synthetic from human text
    \item \textbf{Data Fragility:} Lossy compression or incomplete transmission of stegotext causes irreversible bitstream corruption
\end{itemize}

\subsubsection{Tokenization Mismatch}

Modern Transformer models using \textbf{subword tokenization} (e.g., BPE) suffer from \textbf{segmentation ambiguity}: a sender's token sequence (" any", "thing") may detokenize to "anything" but be \textbf{retokenized differently} by the receiver as a single token, " anything". This breaks the \textbf{autoregressive chain}, corrupting all downstream probability distributions and causing extraction failure. The problem is acute in \textbf{scriptio continua} languages like Chinese, which lack explicit word boundaries.

\textbf{Analogy:} Alice encodes a secret using two small bricks to spell "BLUE." Bob receives one large "BLUE" brick. Since their protocol depends on exact brick counts, Bob's misalignment renders the rest of the message unreadable.

Methods that rely on modifying the sampling probability distribution to embed secret messages inherently introduce security risks because they alter the original distribution, making the steganographic text statistically distinguishable from normal text \cite{yang2020vae, kaptchuk2021meteor, ding2023discop, wu2024generative}. While advancements in deep neural networks have improved text fluency and embedding capacity, older models or certain embedding strategies can still produce texts that lack naturalness, logical coherence, or diversity compared to human-written content. Models like NMT-Stega and Hi-Stega aim to maintain semantic and contextual consistency by leveraging source texts or social media contexts, yet this remains a complex challenge \cite{ding2023context, wang2023hi}.

\textbf{Channel entropy requirements and variability} also pose a considerable challenge. Traditional universal steganographic schemes often demand consistent channel entropy, which is rarely maintained in real-world natural language communication. Moments of low or zero entropy can cause protocols to fail or require extraordinarily long steganographic texts. The Psic Effect highlights this dilemma in balancing quality and detectability.

Additional limitations include:
\begin{itemize}
    \item \textbf{Data Integrity and Reversibility}: Some methods cannot perfectly recover the original cover text after message extraction \cite{zheng2022general, qiang2023natural}.
    \item \textbf{Ethical Concerns}: Pre-trained LLMs may introduce biases, discrimination, or inappropriate content \cite{lin2024zero, bender2021dangers}.
    \item \textbf{Provable Security}: Many NLP steganography works lack rigorous security analyses and fail to meet formal cryptographic definitions \cite{kaptchuk2021meteor}.
\end{itemize}
