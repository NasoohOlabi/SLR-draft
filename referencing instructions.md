To help you add comprehensive citations to your `main.pdf` document, I will identify specific statements or sections from `main.pdf` that draw information from the other sources you provided. For each instance, I will provide the text from `main.pdf` and the corresponding source numbers, along with brief instructions.

Please **bold** the identified text in your `main.pdf` and insert the bracketed citation numbers directly after the relevant statement. If a statement is based on multiple sources, ensure all applicable source numbers are listed.

---

Here are the specific parts of `main.pdf` that are supported by your provided sources:

* **For statements about the overall scope and characteristics of linguistic steganography and LLMs:**
  * **In `main.pdf`'s Abstract:**
    * "This study presents a systematic literature review on textual steganography, with a particular focus on the transformative impact of Large Language Models (LLMs). We trace the evolution of linguistic steganography from early format-based and statistical methods to advanced neural network models, culminating in the current LLM era. Our findings highlight that LLM-based approaches significantly enhance imperceptibility, embedding capacity, and naturalness in cover text generation, addressing long-standing challenges like the Psic Effect."
      * **Instruction:** While this is a summary in `main.pdf`, the underlying concepts of "linguistic steganography," "evolution," "LLM-based approaches enhance imperceptibility, embedding capacity, and naturalness," and "Psic Effect" are discussed in:
        * (Psic Effect, concealment system focus on hiding existence, global information sharing)
        * (Psic Effect, perceptual-imperceptibility, statistical-imperceptibility)
        * (Psic Effect)
        * (linguistic steganography, neural network-based language model, fluency, information embedding capacity)
        * (LLMs favorable for generative text steganography)
        * (modern steganography uses machine learning to enhance imperceptibility)
        * (fluent, highly readable steganographic texts, good anti-steganalysis ability)
        * (linguistic steganography, security, privacy)
        * (linguistic semantic steganography, efficiency, imperceptibility)
        * **Citation for this entire sentence/concept:**
    * "We analyze various LLM-based models, including VAE-Stega, Co-Stega, Discop, and others, detailing their architectures, performance metrics (e.g., BPW, PPL), and unique contributions to the field."
      * **Instruction:** Each of these model mentions can be specifically cited:
        * **VAE-Stega:**
        * **Co-Stega:**
        * **Discop:**
        * **BPW (Bits Per Word):**
        * **PPL (Perplexity):**
      * **Citation for the sentence:**

* **For statements in `main.pdf` Section 1.1 "Overview of Information Security and Concealment Systems":**
  * "Information security systems include encryption, privacy, and concealment (steganography)."
    * **Instruction:** This directly summarizes Claude E. Shannon's work mentioned in:
            *  
    * **Citation:**

* **For statements in `main.pdf` Section 1.1.1 "Encryption Systems and Privacy Systems":**
  * "These protect content but reveal that secret communication is happening, which can attract attention."
    * **Instruction:** This point about encryption systems exposing existence is made in:
      * (expose the existence and importance of the information itself)
    * **Citation:**

* **For statements in `main.pdf` Section 1.1.2 "Concealment Systems (Steganography)":**
  * "Steganography hides the existence of information by embedding it in ordinary carriers (e.g., text, images)."
    * **Instruction:** This definition of steganography is found in:
      * (hiding the existence and thus ensure its security)
      * (embedding secret information into cover media, aiming to covertly transmit secret information through public channels)
    * **Citation:**
  * "Text is a challenging carrier due to its low redundancy and strict semantics."
    * **Instruction:** This challenge for text steganography is explained in:
      * (text has formed very complicatedly semantic coding rules and thus has less semantic ambiguity and information redundancy)
      * (text contains relatively little redundant information (Zipf, 1999))
    * **Citation:**

* **For statements in `main.pdf` Section 1.2 "Introduction to Steganography":**
  * "Steganography is often explained by the “Prisoners’ Problem,”where Alice and Bob must communicate secretly under surveillance. The goal is to embed messages so they are undetectable to an observer."
    * **Instruction:** The "Prisoners' Problem" analogy is a cornerstone of steganography definition, found in:
      * (Simmons reference)
      * (illustrated by Simmons' "Prisoners' Problem")
      * (Simmons reference)
      * (illustrated by Simmons’ "prisoner problem")
      * (Simmons reference)
      * (Simmons reference)
      * (typical example of steganography is the “prisoner problem”)
      * (Simmons reference)
      * (Simmons reference)
      * (illustrated by Simmons’ Prisoners’ Problem)
      * (Simmons reference)
    * **Citation:**

* **For statements in `main.pdf` Section 1.2 "Steganography methods include carrier selection, carrier modification, and carrier generation.":**
  * "Carrier modification: Hide information in existing text with minimal changes."
    * **Instruction:** This method is described in:
      * (steganography methods based on carrier modification...reduce the impact of embedded extra information on the original carrier–)
      * (Modification based linguistic steganog-raphy refers to a special transformation of a given text to realize data embedding.)
    * **Citation:**
  * "Carrier generation: Generate new text that encodes information, allowing higher capacity but requiring naturalness."
    * **Instruction:** This method and its challenges are detailed in:
      * (how to automatically generate a semantic-complete and natural-enough information carrier?)
      * (This generation-based linguistic steganography has great potential in text fluency and information embedding capacity)
      * (generation based methods,–)
    * **Citation:**

* **For statements in `main.pdf` Section 1.3 "The Significance of Linguistic Steganography":**
  * "Linguistic steganography enables covert communication, especially where encryption is suspicious."
    * **Instruction:** This highlights a key advantage over encryption, as noted in:
      * (encryption system and privacy system will also expose the existence and importance of the information itself)
      * (messages transmitted as ciphertexts may easily arouse suspicion from attackers)
    * **Citation:**
  * "Text is a robust, ubiquitous carrier but presents challenges in balancing imperceptibility and capacity."
    * **Instruction:** This summarizes the properties and difficulties of text as a carrier, found in:
      * (text is one of the most important information carriers, playing a very important role in human lives)
      * (transmission robustness of texts...potential in text fluency and information embedding capacity)
      * (text contains relatively little redundant information...embedding large amounts of information can be challenging)
    * **Citation:**
  * "Advances in deep learning and LLMs improve text quality and security, while related fields like watermarking focus on tracing content origin."
    * **Instruction:** This general statement about the benefits of new technologies and the purpose of watermarking is covered by:
      * (Recent advances in large language models (LLMs) have blurred the boundary of high-quality text generation...favorable for generative text steganography)
      * (zero-shot approach...high imperceptibility...stegotext is more intelligible and resembles the covertext)
      * (BERT and Gibbs sampling...generate fluent, highly readable steganographic texts, while enjoying pretty good anti-steganalysis ability)
      * (model increases the embedding capac-ity...while ensuring the feature consistency of the steganographic text to improve imperceptibility)
      * (Watermarking...tracing the prove-nance of the text)
      * (Digital watermarking...provenance tracing...provides ownership evidence)
      * (IP Protection...Provenance Tracking)
      * (Watermarking...tracing text provenance to claim the ownership of text contents)
    * **Citation:**

* **For statements in `main.pdf` Section 2.2 "Role in Generative Linguistic Steganography":**
  * "LLMs are considered favorable for generative text steganography due to their ability to generate high-quality text."
    * **Instruction:** This direct quote is from:
            *  
    * **Citation:**
  * "Researchers propose using generative models as steganographic samplers to embed messages into realistic communication distributions, such as text."
    * **Instruction:** This concept is introduced in:
            *  
    * **Citation:**
  * "This is a departure from prior steganographic work and is motivated by the public availability of high-quality models and significant efficiency gains."
    * **Instruction:** This motivation for using advanced models is clearly stated in:
            *  
    * **Citation:**
  * "LLMs like GPT-2, LLaMA, and Baichuan2 are commonly used as basic generative models for steganography."
    * **Instruction:** These specific models are mentioned in the context of steganography in:
      * **GPT-2:**
      * **LLaMA:**
      * **Baichuan2:**
    * **Citation:**
  * "Existing methods often use a language model and steganographic mapping, where secret messages are embedded by establishing a mapping between binary bits and the sampling probability of words within the training vocabulary."
    * **Instruction:** This describes a common embedding approach, as outlined in:
            *  
    * **Citation:**
  * "However, traditional 'white-box' methods require sharing the exact language model and training vocabulary, which limits fluency, logic, and diversity compared to natural texts generated by LLMs."
    * **Instruction:** This limitation of white-box methods is discussed in:
            *  
    * **Citation:**
  * "They also inevitably change the sampling probability distribution, posing security risks."
    * **Instruction:** This security risk is explicitly mentioned in:
            *  
      * (Stegosampling algorithms will inevitably do damage to the explicit distribution of tokens, and result in a gap between stegotext and covertext.) - Note that and from this source (414) map to and in your provided list.
    * **Citation:**
  * "New approaches, like LLM-Stega, explore black-box generative text steganography using the user interfaces (UIs) of LLMs, overcoming the need to access internal sampling distributions."
    * **Instruction:** The LLM-Stega approach is introduced in:
            *  
    * **Citation:**
  * "This method constructs a keyword set and uses an encrypted steganographic mapping for embedding, proposing an optimization mechanism based on reject sampling for accurate extraction and rich semantics."
    * **Instruction:** These specific features of LLM-Stega are detailed in:
            *  
            *  
            *  
    * **Citation:**
  * "Another framework, Co-Stega, leverages LLMs to address the low capacity challenge in social media by increasing the text space for hiding messages (through context retrieval) and raising the generated text’s entropy via specific prompts to increase embedding capacity."
    * **Instruction:** The Co-Stega framework and its goals are described in:
      * (low capacity challenge)
      * (Retrieval module, Generation module, Entropy Enhancement Strategy)
      * (Entropy Enhancement Strategy using prompts)
    * **Citation:**
  * "This approach also aims to maintain text quality, fluency, and relevance."
    * **Instruction:** The quality aspects of Co-Stega are mentioned in:
            *  
    * **Citation:**
  * "The concept of zero-shot linguistic steganography with LLMs utilizes in-context learning, where samples of covertext are used as context to generate more intelligible stegotext using a question-answer (QA) paradigm."
    * **Instruction:** This describes the core of zero-shot steganography in:
      * (zero-shot approach)
      * (in-context QA tasks)
      * (in-context learning for linguistic steganography)
    * **Citation:**
  * "LLMs are also used in approaches like ALiSa, which directly conceals token-level secret messages in seemingly natural steganographic text generated by off-the-shelf BERT models equipped with Gibbs sampling."
    * **Instruction:** This describes the ALiSa method from:
            *  
    * **Citation:**
  * "The increasing popularity of deep generative models has made it feasible for provably secure steganography to be applied in real-world scenarios, as they fulfill requirements for perfect samplers and explicit data distributions."
    * **Instruction:** This outlines the impact of generative models on provably secure steganography, found in:
            *  
    * **Citation:**

* **For statements in `main.pdf` Section 2.4.3 "Lack of Semantic Control and Contextual Consistency":**
  * "Ensuring generated text matches intended meaning/context is difficult."
    * **Instruction:** This challenge, often related to "cognitive-imperceptibility," is discussed in:
      * (semantics and context of steganographic texts uncontrollable...problem of “cognitive-imperceptibility” in)
      * (semantics and contexts...can be different because of selecting tokens corresponding to the secret message)
    * **Citation:**

* **For statements in `main.pdf` Section 2.4.5 "Segmentation Ambiguity":**
  * "Tokenization can cause ambiguity in how information is embedded or extracted."
    * **Instruction:** The concept of segmentation ambiguity is defined in:
      * (This phenomenon is termed segmentation ambiguity by Nozaki and Murawaki)
      * (a single piece of text can correspond to two or even more different token representations. This phenomenon is referred to as segmentation ambiguity.)
    * **Citation:**

* **For statements in `main.pdf` Section "A primary challenge in steganography, particularly when utiliz-ing Large Language Models (LLMs), revolves around the distinc-tion between white-box and black-box access. Most current advanced generative text steganographic methods operate under a "white-box" paradigm, meaning they require direct access to the LLM’s internal components, such as its training vocabulary and the sampling probabilities of words.":**
  * **Instruction:** The distinction between white-box and black-box and the requirements of white-box methods are described in:
        *  
  * **Citation:**
  * "This presents a significant limitation because many state-of-the-art LLMs are proprietary and are accessed by users primarily through black-box APIs or user interfaces."
    * **Instruction:** This practical limitation is highlighted in:
            *  
    * **Citation:**
  * "Furthermore, methods that rely on modifying the sampling probability distribution to embed secret messages inherently introduce security risks because they alter the original distribution, making the steganographic text statistically distinguishable from normal text."
    * **Instruction:** This inherent security risk is discussed in:
            *  
      * (Stegosampling algorithms will inevitably do damage to the explicit distribution of tokens, and result in a gap between stegotext and covertext.) - Note that and from this source (414) map to and in your provided list.
    * **Citation:**

* **For statements in `main.pdf` Section 5 "LLM-BASED STEGANOGRAPHY APPROACHES":**
  * "Natural Language Steganography by ChatGPT: Uses ChatGPT 4.0. Achieves 0.144 BPWwith natural concealment and scalability."
    * **Instruction:** This sentence describes features and claims about the method. While the exact "0.144 BPW" is not explicitly presented as a single number in the provided snippets of, the method, its use of ChatGPT 4.0, and the concepts of naturalness, concealment, and scalability (implied by LLM capabilities) are central to the paper. Therefore, it is referencing the general findings of:
      * (ChatGPT as source)
      * (initial step...to highlight the feasibility of the approach and to catalyze further, more in-depth research...enhancing robustness, and expanding the practical applications of AI-driven steganographic techniques.)
      * (payload of a text can be increased substantially)
    * **Citation:**
  * "Rewriting-Stego: Uses BART (bart-base2). Achieves 4 BPW with high capacity and naturalness. Key results: BPTS: 4.0, BPTC+S: 4.0, PPL: 62.1."
    * **Instruction:** This summarizes Rewriting-Stego's performance. The model, capacity claim, and specific metrics are found in:
      * (Rewriting-Stego...rewriting-based...higher information capacity without losing naturalness and controllability)
      * (BART as our backbone language model...can hide n-bit secret information in each generated token)
      * (Table 2 shows BPTS, BPTC+S, and PPL values for various Rewriting-Stego settings, including BPTS 4.0 and PPL 62.1 (for BPTS 2.0 on Tweet dataset). It’s important to note the specific PPL value refers to a different BPTS setting in the source table.)
    * **Citation:**
  * "ALiSa (Acrostic Linguistic Steganography): Based on BERT and Gibbs Sampling. Achieves 0.92 BPW. Key results: PPL: Natural = 13.91, ALiSa = 14.85, LS-RNN/LS-BERT Acc & F1 = 0.50."
    * **Instruction:** This summarizes ALiSa's performance and methodology. The model, PPL results, and steganalysis accuracy are found in:
      * (ALiSa...Based on BERT and Gibbs sampling...directly conceals a token-level secret message)
      * (Table I for PPL values; Table III for LS-RNN/LS-BERT Acc & F1 values)
      * Regarding "Achieves 0.92 BPW", the provided source does not explicitly state "0.92 BPW" for ALiSa, but discusses "bits per word" in relation to other methods (GPT-AC, GPT-ADG) and payloads (TPT). It's likely a derived or summarized value.
    * **Citation:**

* **For statements in `main.pdf` Section 6 "CONTEXT AWARENESS":**
  * "Linguistic steganography has evolved from early methods to advanced deep learning models and Large Language Models (LLMs). This progression focuses on improving imperceptibility, embedding capacity, and maintaining naturalness and semantic coherence in cover text."
    * **Instruction:** This is a broad summary of the field's evolution and ongoing goals, supported by information already cited in the Abstract and Introduction sections, covering the general trends.
    * **Citation:**

* **For statements in `main.pdf` Section "The Era of Custom Artificial Neural Network (ANN) Models":**
  * "These neural networks learned language models, encoding secret information by manipulating word probability distributions during generation."
    * **Instruction:** This describes a fundamental mechanism of ANN-based generative steganography, found in:
      * (linguistic steganography often uses a neural network-based lan-guage model to embed secret messages...into each word of generated texts using an en-coding method)
      * (mainstream DH methods applied to texts use a trained language model for facilitating embedding)
      * (primarily utilize neural networks...to learn the statistical language model and then employ the encoding algorithm to encode the conditional probabil-ity distribution of each word in the generation process to embed secret infor-mation)
      * (Generative steganog-raphy algorithm mainly embeds secret information by adjusting and encoding the probability distributions pθ of candidate words.)
      * (generation-based linguistic steganography models actually select the word at each position through secret information rather than semantics or context)
    * **Citation:**
