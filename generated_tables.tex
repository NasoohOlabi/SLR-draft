\begin{table}[htbp]
\centering
\caption{Summary of Results from Reviewed Papers}
\label{tab:results_summary}
\small
\begin{tabularx}{\linewidth}{|X||X|}
\hline
Paper & Result \\\hline
\allowbreak VAE-Stega: linguistic steganography based on variational auto-encoder \allowbreak\cite{yang2020vae} & PPL: 28.879, \ensuremath{\Delta}MP: 0.242, KLD: 3.302, JSD: 10.411, Acc: 0.600, R: 0.616 \\\hline
\allowbreak General framework for reversible data hiding in texts based on masked language modeling \allowbreak\cite{zheng2022general} & BPW=0.5335 F1=0.9402 PPL=134.2199 \\\hline
\allowbreak Co-stega: Collaborative linguistic steganography for the low capacity challenge in social media \allowbreak\cite{liao2024co} & SR1: 60.87\%, SR2: 98.55\%, Gen. Capacity: 44.91 bits, Entropy: 49.21 bits, BPW: 2.31, PPL: 16.75, SimCSE: 0.69 \\\hline
\allowbreak Joint linguistic steganography with BERT masked language model and graph attention network \allowbreak\cite{ding2023joint} & PPL=13.917 KLD=2.904 SIM=0.812 ER=0.365 (BN=2) Best Acc=0.575 (BERT classifier) FLOPs=1.834G \\\hline
\allowbreak Discop: Provably secure steganography in practice based on" distribution copies" \allowbreak\cite{ding2023discop} & p=1.00 Total Time (seconds)=362.63 Ave Time ↓ (seconds/bit)=6.29E-03 Ave KLD ↓ (bits/token)=0 Max KLD ↓ (bits/token)=0 Capacity (bits/token)=5.76 Entropy (bits/token)=6.08 Utilization ↑=0.95 Text Generation (FCN): 50.10\%. Text Generation (R-BiLSTM-C): 50.45\%. Text Generation (BiLSTM-Dense): 49.95\% \\\hline
\allowbreak Generative text steganography with large language model \allowbreak\cite{wu2024generative} & Length: 13.333 (words). BPW: 5.93 bpw PPL: 165.76. Semantic Similarity (SS): 0.5881 LS-CNN Acc: 51.55\%. BiLSTM-Dense Acc: 49.20\%. Bert-FT Acc: 50.00\%. KLD (Log, lower is better): 2.02 . \\\hline
\allowbreak Meteor: Cryptographically secure steganography for realistic distributions \allowbreak\cite{kaptchuk2021meteor} & GPT-2: 3.09 bits/token \\\hline
\allowbreak Zero-shot generative linguistic steganography \allowbreak\cite{lin2024zero} & PPL: 8.81. JSDfull: 17.90 (x10\^{}-2). JSDhalf: 16.86 (x10\^{}-2). JSDzero: 13.40 (x10\^{}-2) TS-BiRNN: 80.29\%. R-BiLSTM-C: 84.34\%. BERT-C: 89.61\% \\\hline
\allowbreak Provably secure disambiguating neural linguistic steganography \allowbreak\cite{qi2024provably} & Total Error: 0\%, Ave KLD: 0, Max KLD: 0, Ave PPL: 3.19 (EN), 7.49 (ZH), Capacity: 1.03–3.05 bits/token, Utilization: 0.66–0.74, Ave Time: ~4\ensuremath{\mu}s/bit \\\hline
\allowbreak A principled approach to natural language watermarking \allowbreak\cite{ji2024principled} & Bit acc: 0.994 (K=None), 1.000 (DAE), 0.978 (Adaptive+K=S); Meteor Drop: ~0.057; SBERT ↑: ~1.227; Ownership Rate: 1.0 (no attack), 0.978 (adaptive+K=S) \\\hline
\allowbreak Context-aware linguistic steganography model based on neural machine translation \allowbreak\cite{ding2023context} & BLEU: 30.5, PPL: 22.5, ER: 0.29, KL: 0.02, SIM: 0.86, Stego detection ~16\% \\\hline
\allowbreak DeepTextMark: a deep learning-driven text watermarking approach for identifying large language model generated text \allowbreak\cite{munyer2024deeptextmark} & 100\% accuracy (multi-synonym, 10-sentence), mSMS: 0.9892, TPR: 0.83, FNR: 0.17, Detection: 0.00188s, Insertion: 0.27931s \\\hline
\allowbreak Hi-stega: A hierarchical linguistic steganography framework combining retrieval and generation \allowbreak\cite{wang2023hi} & ppl: 109.60, MAUVE: 0.2051, ER2: 10.42, \ensuremath{\Delta}(cosine): 0.0088, \ensuremath{\Delta}(simcse): 0.0191 \\\hline
\allowbreak Linguistic steganography: From symbolic space to semantic space \allowbreak\cite{zhang2020linguistic} & Classifier Accuracy: 0.9880; Loop Count: 1.0160; PPL: 13.9565; Anti-Steganalysis Accuracy: ~0.5 \\\hline
\allowbreak Natural language steganography by chatgpt \allowbreak\cite{steinebach2024natural} & [Not specified] \\\hline
\allowbreak Natural language watermarking via paraphraser-based lexical substitution \allowbreak\cite{qiang2023natural} & LS07 P@1: 58.3, GAP: 65.1; CoInCo P@1: 62.6, GAP: 60.7; Text Recoverability: ~88–90\% \\\hline
\allowbreak Rewriting-Stego: generating natural and controllable steganographic text with pre-trained language model \allowbreak\cite{li2023rewriting} & BPTS: 4.0, BPTC+S: 4.0, PPL: 62.1, Mean: 44.4, Variance: 2.1e04, Acc: 8.9\% \\\hline
\allowbreak ALiSa: Acrostic linguistic steganography based on BERT and Gibbs sampling \allowbreak\cite{yi2022alisa} & PPL: Natural = 13.91, ALiSa = 14.85; LS-RNN/LS-BERT Acc \& F1 = ~0.50; Outperforms GPT-AC/ADG in all cases \\\hline
\end{tabularx}
\end{table}

\begin{table*}[htbp]
\centering
\caption{Models and Datasets Used in Reviewed Papers}
\label{tab:models_datasets}
\small
\begin{tabularx}{\linewidth}{|X||X||X|}
\hline
Paper & Llm & Dataset \\\hline
\allowbreak VAE-Stega: linguistic steganography based on variational auto-encoder \allowbreak\cite{yang2020vae} & BERTBASE (BERT-LSTM)  (LSTM-LSTM) model was trained from scratch & Twitter (2.6M sentences) IMDB (1.2M sentences) preprocessed \\\hline
\allowbreak General framework for reversible data hiding in texts based on masked language modeling \allowbreak\cite{zheng2022general} & BERTBase & BookCorpus \\\hline
\allowbreak Co-stega: Collaborative linguistic steganography for the low capacity challenge in social media \allowbreak\cite{liao2024co} & Llama-2-7B-chat, GPT-2 (fine-tuned), Llama-2-13B & Tweet dataset (for GPT-2 fine-tuning), Twitter (real-time testing) \\\hline
\allowbreak Joint linguistic steganography with BERT masked language model and graph attention network \allowbreak\cite{ding2023joint} & LSTM + attention for temporal context. GAT for spatial token relationships. BERT MLM for deep semantic context in substitution. & OPUS \\\hline
\allowbreak Discop: Provably secure steganography in practice based on" distribution copies" \allowbreak\cite{ding2023discop} & GPT-2 & IMDB \\\hline
\allowbreak Generative text steganography with large language model \allowbreak\cite{wu2024generative} & Any & [Not specified] \\\hline
\allowbreak Meteor: Cryptographically secure steganography for realistic distributions \allowbreak\cite{kaptchuk2021meteor} & GPT-2 & Hutter Prize, HTTP GET requests \\\hline
\allowbreak Zero-shot generative linguistic steganography \allowbreak\cite{lin2024zero} & LLaMA2-Chat-7B (as the stegotext generator / QA model). GPT-2 (for NLS baseline and JSD evaluation) & IMDB, Twitter \\\hline
\allowbreak Provably secure disambiguating neural linguistic steganography \allowbreak\cite{qi2024provably} & LLaMA2-7b (English), Baichuan2-7b (Chinese) & IMDb dataset (100 texts/sample, 3 English sentences + Chinese translations) \\\hline
\allowbreak A principled approach to natural language watermarking \allowbreak\cite{ji2024principled} & Transformer-based encoder/decoder; BERT for distillation & Web Transformer 2 \\\hline
\allowbreak Context-aware linguistic steganography model based on neural machine translation \allowbreak\cite{ding2023context} & BERT (encoder), LSTM (decoder) & WMT18 News Commentary (train/test), Yang et al. bits, Doc2Vec, 5,000 stego pairs (8:1:1 split) \\\hline
\allowbreak DeepTextMark: a deep learning-driven text watermarking approach for identifying large language model generated text \allowbreak\cite{munyer2024deeptextmark} & Model-independent; tested with OPT-2.7B & Dolly ChatGPT (train/validate), C4 (test), robustness \& sentence-level test sets \\\hline
\allowbreak Hi-stega: A hierarchical linguistic steganography framework combining retrieval and generation \allowbreak\cite{wang2023hi} & GPT-2 & Yahoo! News (titles, bodies, comments); 2,400 titles used \\\hline
\allowbreak Linguistic steganography: From symbolic space to semantic space \allowbreak\cite{zhang2020linguistic} & CTRL (generation), BERT (semantic classifier) & 5,000 CTRL-generated texts per semanteme (n = 2–16); 1,000 user-generated texts for anti-steganalysis \\\hline
\allowbreak Natural language steganography by chatgpt \allowbreak\cite{steinebach2024natural} & [Not specified] & Custom word sets for specific topics (e.g., 16×10-word sets for music reviews) \\\hline
\allowbreak Natural language watermarking via paraphraser-based lexical substitution \allowbreak\cite{qiang2023natural} & Transformer (Paraphraser), BART (BARTScore), BERT (BLEURT, comparisons) & ParaBank2, LS07, CoInCo, Novels, WikiText-2, IMDB, NgNews \\\hline
\allowbreak Rewriting-Stego: generating natural and controllable steganographic text with pre-trained language model \allowbreak\cite{li2023rewriting} & BART (bart-base2) & Movie, News, Tweet \\\hline
\allowbreak ALiSa: Acrostic linguistic steganography based on BERT and Gibbs sampling \allowbreak\cite{yi2022alisa} & BERT (Google’s BERTBase, Uncased) & BookCorpus (10,000 natural texts for evaluation) \\\hline
\end{tabularx}
\end{table*}

\begin{table*}[htbp]
\centering
\caption{Context-Related Fields in Reviewed Papers}
\label{tab:context_fields}
\small
\begin{tabularx}{\linewidth}{|X||X||X||X||X|}
\hline
Paper & Context Aware & Categ Context & Representation Context & Context Usage In Method Detail Text \\\hline
\allowbreak VAE-Stega: linguistic steganography based on variational auto-encoder \allowbreak\cite{yang2020vae} & non-explicit & pre-text & text & [Not specified] \\\hline
\allowbreak General framework for reversible data hiding in texts based on masked language modeling \allowbreak\cite{zheng2022general} & non-explicit & pre-text & text & [Not specified] \\\hline
\allowbreak Co-stega: Collaborative linguistic steganography for the low capacity challenge in social media \allowbreak\cite{liao2024co} & explicit & Social Media & text & Context is used for embedding and generation: the retrieval module embeds part of the message by selecting posts matching the query, and the generation module encodes the rest in replies conditioned on that context to balance capacity and relevance. \\\hline
\allowbreak Joint linguistic steganography with BERT masked language model and graph attention network \allowbreak\cite{ding2023joint} & explicit & pre-text & text & [Not specified] \\\hline
\allowbreak Discop: Provably secure steganography in practice based on" distribution copies" \allowbreak\cite{ding2023discop} & non-explicit & tuning + pretext & text & [Not specified] \\\hline
\allowbreak Generative text steganography with large language model \allowbreak\cite{wu2024generative} & explicit & [Not specified] & [Not specified] & [Not specified] \\\hline
\allowbreak Meteor: Cryptographically secure steganography for realistic distributions \allowbreak\cite{kaptchuk2021meteor} & non-explicit & tuning + pretext & text & [Not specified] \\\hline
\allowbreak Zero-shot generative linguistic steganography \allowbreak\cite{lin2024zero} & explicit & zero-shot + prompt & text & [Not specified] \\\hline
\allowbreak Provably secure disambiguating neural linguistic steganography \allowbreak\cite{qi2024provably} & non-explicit & pretext & text & pre-text \\\hline
\allowbreak A principled approach to natural language watermarking \allowbreak\cite{ji2024principled} & Yes; semantic-level embedding; synonym substitution using BERT & Yes; watermark message assigned categorical label (e.g., 4-bit → 1-of-16) & Yes; semantic embeddings via transformer encoder and BERT; SBERT distance as metric & [Not specified] \\\hline
\allowbreak Context-aware linguistic steganography model based on neural machine translation \allowbreak\cite{ding2023context} & Yes & [Not specified] & GCF (global context), LMR (language model reference), Multi-head attention & [Not specified] \\\hline
\allowbreak DeepTextMark: a deep learning-driven text watermarking approach for identifying large language model generated text \allowbreak\cite{munyer2024deeptextmark} & NO & [Not specified] & [Not specified] & [Not specified] \\\hline
\allowbreak Hi-stega: A hierarchical linguistic steganography framework combining retrieval and generation \allowbreak\cite{wang2023hi} & explicit & Social Media & Text & The method retrieves a context data carrier from a social corpus, using it as input to the language model. This context embeds some secret content naturally, while a Prompt Learning Module ensures template-based, semantically coherent stegotext generation. This boosts realism and imperceptibility in social settings. \\\hline
\allowbreak Linguistic steganography: From symbolic space to semantic space \allowbreak\cite{zhang2020linguistic} & implicit & Text & Semanteme (\ensuremath{\alpha}) as a vector in semantic spac & The semanteme (\ensuremath{\alpha}) shapes the generated text to match a topic, ensuring it appears natural and blends into regular communication. This contextual alignment helps conceal the message and enables the classifier to recover it using the same semantic grounding. \\\hline
\allowbreak Natural language steganography by chatgpt \allowbreak\cite{steinebach2024natural} & Explicit & Specific Genre/Topic Text & Text & The system uses the topic as core context. The Key Generator builds keyword sets typical of the topic, and the Embedder ensures generated text appears natural and topic-appropriate. ChatGPT’s contextual capabilities maintain coherence while embedding messages seamlessly. \\\hline
\allowbreak Natural language watermarking via paraphraser-based lexical substitution \allowbreak\cite{qiang2023natural} & Explicit & [Not specified] & text & ParaLS uses context to generate semantically appropriate substitutions. Decoder predictions begin with prefix words, and attention uses cumulative source weights. Semantic similarity scores rank substitutes, ensuring watermarked sentences retain meaning. Context ensures reproducibility of embedding and extraction through identical candidate sets. \\\hline
\allowbreak Rewriting-Stego: generating natural and controllable steganographic text with pre-trained language model \allowbreak\cite{li2023rewriting} & not Explicit & [Not specified] & [Not specified] & [Not specified] \\\hline
\allowbreak ALiSa: Acrostic linguistic steganography based on BERT and Gibbs sampling \allowbreak\cite{yi2022alisa} & No & [Not specified] & [Not specified] & [Not specified] \\\hline
\end{tabularx}
\end{table*}

\begin{table*}[htbp]
\centering
\caption{Categories, Main Strengths, and Weaknesses of Reviewed Papers}
\label{tab:category_strengths_weaknesses}
\small
\begin{tabularx}{\linewidth}{|X||X||X||X|}
\hline
Paper & Type & Main Strengths & Main Weaknesses \\\hline
\allowbreak VAE-Stega: linguistic steganography based on variational auto-encoder \allowbreak\cite{yang2020vae} & Steganography & statistical fidelity & Not AutoRegressive \\\hline
\allowbreak General framework for reversible data hiding in texts based on masked language modeling \allowbreak\cite{zheng2022general} & Steganography & [Not specified] & [Not specified] \\\hline
\allowbreak Co-stega: Collaborative linguistic steganography for the low capacity challenge in social media \allowbreak\cite{liao2024co} & Steganography & High capacity, fluency, semantic relevance, EES boosts entropy, \ensuremath{\epsilon}-security satisfied & Context limits capacity, retrieval vulnerable to mismatch, small models limit query handling \\\hline
\allowbreak Joint linguistic steganography with BERT masked language model and graph attention network \allowbreak\cite{ding2023joint} & Steganography & [Not specified] & [Not specified] \\\hline
\allowbreak Discop: Provably secure steganography in practice based on" distribution copies" \allowbreak\cite{ding2023discop} & Steganography & [Not specified] & [Not specified] \\\hline
\allowbreak Generative text steganography with large language model \allowbreak\cite{wu2024generative} & Steganography & [Not specified] & [Not specified] \\\hline
\allowbreak Meteor: Cryptographically secure steganography for realistic distributions \allowbreak\cite{kaptchuk2021meteor} & Steganography & [Not specified] & [Not specified] \\\hline
\allowbreak Zero-shot generative linguistic steganography \allowbreak\cite{lin2024zero} & Steganography & [Not specified] & [Not specified] \\\hline
\allowbreak Provably secure disambiguating neural linguistic steganography \allowbreak\cite{qi2024provably} & Steganography & Zero decoding error, provable security, perceptual imperceptibility, multilingual support, transferable design & Reduced entropy utilization due to token prefix loss in detokenization/retokenization \\\hline
\allowbreak A principled approach to natural language watermarking \allowbreak\cite{ji2024principled} & Watermarking & [Not specified] & [Not specified] \\\hline
\allowbreak Context-aware linguistic steganography model based on neural machine translation \allowbreak\cite{ding2023context} & Steganography & Context control, Semantic match & Old models lacked context, low capacity (waiting), probabilistic cliffs, possible meaning mismatch \\\hline
\allowbreak DeepTextMark: a deep learning-driven text watermarking approach for identifying large language model generated text \allowbreak\cite{munyer2024deeptextmark} & Watermarking & Blindness, robustness, imperceptibility, high accuracy, automatic, model-independent, compact, fast, sentence-level & Training data dependence, needs pre-watermarking, limited on short/diverse texts, insertion overhead, rewrite circumvention \\\hline
\allowbreak Hi-stega: A hierarchical linguistic steganography framework combining retrieval and generation \allowbreak\cite{wang2023hi} & Steganography & High payload, semantic coherence, high text quality, strong resistance to steganalysis & Slightly reduced diversity, limited retrieval scope \\\hline
\allowbreak Linguistic steganography: From symbolic space to semantic space \allowbreak\cite{zhang2020linguistic} & Steganography & Symbol-free, efficient, imperceptible, accurate extraction, complements symbolic steganography & Classifier struggles with large n, higher loop count, semanteme text quality varies, orthogonality issues \\\hline
\allowbreak Natural language steganography by chatgpt \allowbreak\cite{steinebach2024natural} & Steganography & Effective LLM-based stego text, natural concealment, scalable, supports prompt iteration & NL complexity, duplication, hallucinations, errors in long texts, statistical artifacts \\\hline
\allowbreak Natural language watermarking via paraphraser-based lexical substitution \allowbreak\cite{qiang2023natural} & Watermarking & High meaning preservation, strong performance vs. BERT-based NLW, no annotated data needed, robust decoding, high text recoverability & Limited reversibility, hard for short texts, beam decoding limits substitution, lower capacity in some cases \\\hline
\allowbreak Rewriting-Stego: generating natural and controllable steganographic text with pre-trained language model \allowbreak\cite{li2023rewriting} & Steganography & High capacity, naturalness, reduced vigilance, denoising, real-world efficiency & Similarity drops with high BPTS, may underperform in detection accuracy vs edit-based \\\hline
\allowbreak ALiSa: Acrostic linguistic steganography based on BERT and Gibbs sampling \allowbreak\cite{yi2022alisa} & Steganography & Simple & Poor Quality, low cap \\\hline
\end{tabularx}
\end{table*}

\begin{table*}[htbp]
\centering
\caption{Approach/Pipeline Method Used in Reviewed Papers}
\label{tab:approach_method}
\small
\begin{tabularx}{\linewidth}{|X||X|}
\hline
Paper & Pipline Method Used \\\hline
\allowbreak VAE-Stega: linguistic steganography based on variational auto-encoder \allowbreak\cite{yang2020vae} & Input text → Encoder (LSTM/BERT) → latent vector z ~ N(0,1) z → Decoder (LSTM) → conditional distribution → top-m Candidate Pool Candidate Pool + secret bits → Coding (Huffman/Arithmetic) → word output Repeat → sentence with hidden bits Shared z + Decoder → conditional distribution → CP reconstruction Observed word + CP → decode bits \\\hline
\allowbreak General framework for reversible data hiding in texts based on masked language modeling \allowbreak\cite{zheng2022general} & Initialized text (secret words fixed, others masked) → Masked LM → conditional distribution over masked slots → secret bits guide word selection → Marked Text Marked Text = original words + data-encoded generated words → Receiver (with key) → decode secret bits + recover original text \\\hline
\allowbreak Co-stega: Collaborative linguistic steganography for the low capacity challenge in social media \allowbreak\cite{liao2024co} & Message → split into query bits + remaining bits   Query bits → Search engine → retrieve matching posts   Remaining bits + retrieved context → LLM (with entropy prompts) → generate reply   → Reply embeds remaining bits \\\hline
\allowbreak Joint linguistic steganography with BERT masked language model and graph attention network \allowbreak\cite{ding2023joint} & pre‑text → LSTM → Temporal context (H\_time) → self-attention → H\_time'                                   Temporal context (H\_time) + sliding-window graph                         → GAT → Spatial context (H\_space)  GAT + H\_time' + H\_space → fusion → next-token distribution \\\hline
\allowbreak Discop: Provably secure steganography in practice based on" distribution copies" \allowbreak\cite{ding2023discop} & sampling using the distribution copy \\\hline
\allowbreak Generative text steganography with large language model \allowbreak\cite{wu2024generative} & Keyword Prompt → LLM → 4 keyword subsets: - Subject, Predicate, Object (16 words each) - Emotion (3 words: negative, neutral, positive) → Evaluation Prompt → LLM optimizes sampling probabilities \\\hline
\allowbreak Meteor: Cryptographically secure steganography for realistic distributions \allowbreak\cite{kaptchuk2021meteor} & Next-word distribution → partitioned into intervals → random r selects word → shared r prefix → message bits → r XOR PRG mask → secure transmission → Receiver unmasks r → decodes bits → Bit rate \ensuremath{\propto} entropy of next word \\\hline
\allowbreak Zero-shot generative linguistic steganography \allowbreak\cite{lin2024zero} & Secret text → Huffman + Edge Flipping → binary bitstream   → LM generates candidates (filtered by threshold \ensuremath{\tau})   → Embedding Module selects word matching bitstream prefix   + Annealing Selection (temperature control)   + Repeat Penalty (discourages repetition)   → Word output per step → encoded sentence \\\hline
\allowbreak Provably secure disambiguating neural linguistic steganography \allowbreak\cite{qi2024provably} & Context → token distribution prediction → ambiguous token pooling → CSPRNG + message bits select tokens from pools → detokenize → coherent stegotext \\\hline
\allowbreak A principled approach to natural language watermarking \allowbreak\cite{ji2024principled} & Original Text (S), Watermark bits (m), Side info (K) → Encoder (Transformer) → paraphrase under distortion constraints → Watermarked Text (X) Attacker (omniscient) → corrupts X → Corrupted Text (Y) Decoder (Transformer + linear) + K → estimate watermark (m) Train via rate-distortion game maximizing mutual info, minimizing distortion, with adversarial robustness \\\hline
\allowbreak Context-aware linguistic steganography model based on neural machine translation \allowbreak\cite{ding2023context} & Context (C), Secret Message (m), Shared Key (K) → Language Model (Transformer) → next-token distribution (Pc, V) SyncPool Ambiguity Pool Grouping → ambiguous tokens grouped (V\_amb), preserving original distribution Embed message via ENCODE + CSPRNGsteg → select Ambiguity Pool If ambiguous → CSPRNGsync → synchronous token selection Detokenize → Stegotext (S) → transmit Receiver DECODE + retokenization + CSPRNGsteg + CSPRNGsync → extract message bits, preserving provable security \\\hline
\allowbreak DeepTextMark: a deep learning-driven text watermarking approach for identifying large language model generated text \allowbreak\cite{munyer2024deeptextmark} & Input text → Identify candidate words (exclude stopwords/punctuation) Candidates → Word2Vec → top-n semantically similar words Secret bits + candidates → Guided substitution → sentence proposals Proposals → USE → select highest similarity → Marked Text Marked Text = original + synonym substitutions (watermarked) Marked Text → BERT-based classifier → detect watermark (blind detection) \\\hline
\allowbreak Hi-stega: A hierarchical linguistic steganography framework combining retrieval and generation \allowbreak\cite{wang2023hi} & Secret message m → split into data info + control info Data info → retrieve carrier c from corpus C → maximize m \ensuremath{\cap} c Control info = (keywords k = m \ c, positions i = m \ensuremath{\cap} c) → format → bitstream b Bitstream b + keywords k + carrier c → Prompt Learning + Keyword Guidance → generate stegotext s s embeds keywords k + hidden bitstream b → high semantic coherence (\ensuremath{\Delta}cos, \ensuremath{\Delta}simcse) → cognitive imperceptibility Only c or s \ensuremath{\neq} full m → layer separation → security ↑, anti-steganalysis ↑ Embedding rate (ER2) ↑ → high payload per word with low perplexity \\\hline
\allowbreak Linguistic steganography: From symbolic space to semantic space \allowbreak\cite{zhang2020linguistic} & Secret message → Semantic mapping → semanteme \ensuremath{\alpha} (discrete space, log(n)-bit) \ensuremath{\alpha} → CTRL (Transformer) → stegotext x (conditional distribution p\ensuremath{\theta}(x|\ensuremath{\alpha})) Stegotext x → BERT classifier → extracted semanteme \ensuremath{\alpha}' \ensuremath{\alpha}' \ensuremath{\neq} \ensuremath{\alpha} → Rejection sampling → repeat generation until \ensuremath{\alpha}' = \ensuremath{\alpha} Final stegotext → Bob decodes → secret message m' \\\hline
\allowbreak Natural language steganography by chatgpt \allowbreak\cite{steinebach2024natural} & Key Generator: Topic-specific word sets → labeled subsets (e.g., A, B) → no duplicates, single words only Message → sequence of subset labels (stegosequence, e.g., "AABBAABB") Embedder (ChatGPT 4.0): Stegosequence + topic + word sets → stego cover text \\\hline
\allowbreak Natural language watermarking via paraphraser-based lexical substitution \allowbreak\cite{qiang2023natural} & Original sentence → Paraphraser (Transformer) → paraphrases → extract substitute candidates for target word xi Novel decoding → force prefix x<i → predict vocabulary distribution for yi → top-K substitutes Substitutes → rank by Semantic Textual Similarity (BARTScore + BLEURT + prediction scores) → select best candidate Exchangeability test → ensure xi and yi are mutually interchangeable → form candidate set C = {c0, c1} Watermark bit (0/1) → replace xi with csignal from C → watermarked sentence x' Extraction → mirror embedding → generate C for x' → compare xi with c0/c1 → recover watermark bit \\\hline
\allowbreak Rewriting-Stego: generating natural and controllable steganographic text with pre-trained language model \allowbreak\cite{li2023rewriting} & Cover text (Y) + Conditional Code → BART Encoder → encoded representation Secret message (S) → bit stream → 2\^{}n vocabulary groups, each with n-bit code BART Decoder + Group-Wise Masked Decoding → mask non-aligned token probabilities → stego text (Y') Beam search → K candidate stego texts GPT2 → Perplexity (PPL) scoring → select lowest PPL candidate as final stego text \\\hline
\allowbreak ALiSa: Acrostic linguistic steganography based on BERT and Gibbs sampling \allowbreak\cite{yi2022alisa} & Secret tokens → initialize stego text s with [MASK] tokens Secret plaintext w → insert tokens at positions p in s Remaining [MASK] tokens → BERT MLM → probability distribution Gibbs sampling → sample tokens from conditional distribution → fill s Iterate T times → refine non-secret positions → fluent stego text BERT + Gibbs → low Perplexity (PPL) → high fluency Conditional sampling → distribution mimics natural text → strong anti-steganalysis Receiver → extract plaintext from positions p \\\hline
\end{tabularx}
\end{table*}

