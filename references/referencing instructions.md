Based on your query, here are specific instructions for citing information in `main.pdf` that is drawn from the other sources you provided:

To properly cite the content in `main.pdf` that originates from other sources, you should **add the citation key in brackets `[ ]` immediately after the statement or phrase that is supported by that source**. If a statement is supported by multiple sources, list all relevant keys separated by commas.

Here's a breakdown of the content in `main.pdf` (specifically pages 555-577) that can be referenced to your provided sources, along with clear instructions on where to add the citations:

---

### Instructions for Adding References to `main.pdf`

**1. Section 2.2 Role in Generative Linguistic Steganography (Page 560):**

* **Original Text in `main.pdf`:** "New approaches, like LLM-Stega, explore black-box generative text steganography using the user interfaces (UIs) of LLMs, overcoming the need to access internal sampling distributions. This method constructs a keyword set and uses an encrypted steganographic mapping for embedding, proposing an optimization mechanism based on reject sampling for accurate extraction and rich semantics."
  * **Instruction:** Append `[wu2024generative]` after the last sentence.
  * **Result:** "...This method constructs a keyword set and uses an encrypted steganographic mapping for embedding, proposing an optimization mechanism based on reject sampling for accurate extraction and rich semantics **[wu2024generative]**."

* **Original Text in `main.pdf`:** "Another framework, Co-Stega, leverages LLMs to address the low capacity challenge in social media by increasing the text space for hiding messages (through context retrieval) and raising the generated text’s entropy via specific prompts to increase embedding capacity. This approach also aims to maintain text quality, fluency, and relevance."
  * **Instruction:** Append `[liao2024co]` after the last sentence.
  * **Result:** "...This approach also aims to maintain text quality, fluency, and relevance **[liao2024co]**."

* **Original Text in `main.pdf`:** "The concept of zero-shot linguistic steganography with LLMs utilizes in-context learning, where samples of covertext are used as context to generate more intelligible stegotext using a question-answer (QA) paradigm."
  * **Instruction:** Append `[lin2024zero]` after the sentence.
  * **Result:** "...using a question-answer (QA) paradigm **[lin2024zero]**."

* **Original Text in `main.pdf`:** "LLMs are also used in approaches like ALiSa, which directly conceals token-level secret messages in seemingly natural steganographic text generated by off-the-shelf BERT models equipped with Gibbs sampling."
  * **Instruction:** Append `[yi2022alisa]` after the sentence.
  * **Result:** "...equipped with Gibbs sampling **[yi2022alisa]**."

* **Original Text in `main.pdf`:** "The increasing popularity of deep generative models has made it feasible for provably secure steganography to be applied in real-world scenarios, as they fulfill requirements for perfect samplers and explicit data distributions."
  * **Instruction:** Append `[ding2023discop, kaptchuk2021meteor, qi2024provably]` after the sentence.
  * **Result:** "...explicit data distributions **[ding2023discop, kaptchuk2021meteor, qi2024provably]**."

**2. Section 2.4 Challenges and Limitations in Steganography with LLMs (Pages 561-567):**

* **Original Text in `main.pdf`:** "Most current advanced generative text steganographic methods operate under a "white-box" paradigm, meaning they require direct access to the LLM’s internal components, such as its training vocabulary and the sampling probabilities of words. This presents a significant limitation because many state-of-the-art LLMs are proprietary and are accessed by users primarily through black-box APIs or user interfaces."
  * **Instruction:** Append `[wu2024generative]` after the second sentence.
  * **Result:** "...APIs or user interfaces **[wu2024generative]**."

* **Original Text in `main.pdf`:** "Furthermore, methods that rely on modifying the sampling probability distribution to embed secret messages inherently introduce security risks because they alter the original distribution, making the steganographic text statistically distinguishable from normal text."
  * **Instruction:** Append `[yang2020vae, kaptchuk2021meteor, ding2023discop, wu2024generative]` after the sentence.
  * **Result:** "...distinguishable from normal text **[yang2020vae, kaptchuk2021meteor, ding2023discop, wu2024generative]**."

* **Original Text in `main.pdf`:** "Linguistic steganography methods often struggle to control the semantics and contextual characteristics of the generated text, leading to a decline in its "cognitive-imperceptibility"."
  * **Instruction:** Append `[yang2020vae, ding2023context]` after the sentence.
  * **Result:** "...cognitive-imperceptibility **[yang2020vae, ding2023context]**."

* **Original Text in `main.pdf`:** "Although models like NMT-Stega and Hi-Stega aim to maintain semantic and contextual consistency by leveraging source texts or social media contexts, this remains a complex challenge."
  * **Instruction:** Append `[ding2023context, wang2023hi]` after the sentence.
  * **Result:** "...this remains a complex challenge **[ding2023context, wang2023hi]**."

* **Original Text in `main.pdf`:** "Furthermore, segmentation ambiguity introduced by subword-based language models, commonly used in high-performing Transformer architectures, presents a critical issue for provably secure linguistic steganography. When a sender detokenizes generated subword sequences into a continuous text (e.g., "any" + "thing" becoming "anything") before transmission, the receiver might retokenize it differently (e.g., as a single "anything" token), leading to decoding errors and affecting subsequent probability distributions. Existing disambiguation solutions typically involve modifying the token candidate pool or probability distributions, which renders them incompatible with the strict requirements of provably secure steganography that demand unchanged distributions."
  * **Instruction:** Append `[qi2024provably]` after the last sentence in this paragraph.
  * **Result:** "...that demand unchanged distributions **[qi2024provably]**."

* **Original Text in `main.pdf`:** "While SyncPool attempts to address this without altering the distribution, it may still lead to a reduction in the embedding rate due to information loss."
  * **Instruction:** Append `[qi2024provably]` after the sentence.
  * **Result:** "...due to information loss **[qi2024provably]**."

* **Original Text in `main.pdf` (Computational Overhead):** "LLMs, while powerful, incur a higher computational cost (3-5 times more than prior methods), which could impact real-time communication scenarios."
  * **Instruction:** Append `[lin2024zero]` after the sentence.
  * **Result:** "...communication scenarios **[lin2024zero]**."

* **Original Text in `main.pdf` (Data Integrity and Reversibility):** "Some linguistic steganography methods are not reversible, meaning the original cover text cannot be perfectly recovered after message extraction, which is undesirable for sensitive applications. Text data is generally less prone to lossy compression issues than other media, but incompleteness of the steganographic text can still damage the embedded bitstream."
  * **Instruction:** For the first sentence, append `[zheng2022general, qiang2023natural]` to indicate methods that are or are not reversible. For the second sentence, append `[lin2024zero]` for the part about damage to the embedded bitstream due to incompleteness.
  * **Result:** "Some linguistic steganography methods are not reversible, meaning the original cover text cannot be perfectly recovered after message extraction, which is undesirable for sensitive applications **[zheng2022general, qiang2023natural]**. Text data is generally less prone to lossy compression issues than other media, but incompleteness of the steganographic text can still damage the embedded bitstream **[lin2024zero]**."

* **Original Text in `main.pdf` (Ethical Concerns):** "The use of pre-trained LLMs may inadvertently introduce ethical issues such as political biases, gender discrimination, or the generation of insulting content."
  * **Instruction:** Append `[lin2024zero]` after the sentence.
  * **Result:** "...insulting content **[lin2024zero]**."

* **Original Text in `main.pdf` (Provable Security and Rigor):** "Despite decades of research into provably secure steganography, practical systems have been hampered by strict requirements like perfect samplers and explicit data distributions."
  * **Instruction:** Append `[ding2023discop, kaptchuk2021meteor]` after the sentence.
  * **Result:** "...explicit data distributions **[ding2023discop, kaptchuk2021meteor]**."

* **Original Text in `main.pdf`:** "Many works from the NLP community, while generating convincing text, often lack rigorous security analyses and fail to meet formal cryptographic definitions, making them vulnerable to detection."
  * **Instruction:** Append `[kaptchuk2021meteor]` after the sentence.
  * **Result:** "...vulnerable to detection **[kaptchuk2021meteor]**."

* **Original Text in `main.pdf`:** "When using approximate samplers, there’s a risk that an adversary can detect a steganographic message by distinguishing between the real channel and the approximation."
  * **Instruction:** Append `[kaptchuk2021meteor]` after the sentence.
  * **Result:** "...and the approximation **[kaptchuk2021meteor]**."

* **Original Text in `main.pdf`:** "The effectiveness of LLM-based steganography can be limited by the entropy of the cover text in social media contexts, as short, context-dependent replies have lower entropy, thus limiting hiding capacity."
  * **Instruction:** Append `[liao2024co]` after the sentence.
  * **Result:** "...limiting hiding capacity **[liao2024co]**."

**3. Section 5.1 LLM-Based Steganography Models (Pages 570-575):**

* **Original Text in `main.pdf`:** "VAE-Stega: Utilizes Variational Auto-Encoders with BERT-BASE (BERT-LSTM) or LSTM-LSTM models. Achieves a bit per word (BPW) of 5.245, focusing on statistical fidelity **[yang2020vae]**. Evaluated using PPL, KLD, JSD, FE, CNN, and TS-CSW (Acc, R). Key results include PPL: 28.879, KLD: 3.302, JSD: 10.411, Acc: 0.600, R: 0.616."
  * **Instruction:** Append `[yang2020vae]` after the first sentence of the bullet point.
  * **Result:** "VAE-Stega: Utilizes Variational Auto-Encoders with BERT-BASE (BERT-LSTM) or LSTM-LSTM models. Achieves a bit per word (BPW) of 5.245, focusing on statistical fidelity **[yang2020vae]**. Evaluated using PPL, KLD, JSD, FE, CNN, and TS-CSW (Acc, R). Key results include PPL: 28.879, KLD: 3.302, JSD: 10.411, Acc: 0.600, R: 0.616."

* **Original Text in `main.pdf`:** "General Framework for Reversible Data Hiding: Employs BERTBase for Masked Language Modeling (MLM). Achieves a BPW of 4.152 **[zheng2022general]**. Key results include BPW=0.5335, F1=0.9402, PPL=134.2199."
  * **Instruction:** Append `[zheng2022general]` after the first sentence of the bullet point.
  * **Result:** "General Framework for Reversible Data Hiding: Employs BERTBase for Masked Language Modeling (MLM). Achieves a BPW of 4.152 **[zheng2022general]**. Key results include BPW=0.5335, F1=0.9402, PPL=134.2199."

* **Original Text in `main.pdf`:** "Co-Stega: Leverages Llama-2-7B-chat, GPT-2, and Llama-2-13B. Demonstrates high capacity (10.42 BPW), fluency, semantic relevance, and strong resistance to steganalysis **[liao2024co]**. Key results include SR1: 60.87"
  * **Instruction:** Append `[liao2024co]` after the first sentence of the bullet point.
  * **Result:** "Co-Stega: Leverages Llama-2-7B-chat, GPT-2, and Llama-2-13B. Demonstrates high capacity (10.42 BPW), fluency, semantic relevance, and strong resistance to steganalysis **[liao2024co]**. Key results include SR1: 60.87"

* **Original Text in `main.pdf`:** "Joint Linguistic Steganography: Combines BERT MLM with Graph Attention Networks (GAT). Yields a BPW of 2.251 and a PPL of 13.917 **[ding2023joint]**. Other metrics include KLD=2.904, SIM=0.812, ER=0.365."
  * **Instruction:** Append `[ding2023joint]` after the first sentence of the bullet point.
  * **Result:** "Joint Linguistic Steganography: Combines BERT MLM with Graph Attention Networks (GAT). Yields a BPW of 2.251 and a PPL of 13.917 **[ding2023joint]**. Other metrics include KLD=2.904, SIM=0.812, ER=0.365."

* **Original Text in `main.pdf`:** "Discop (Provably Secure Steganography): Uses GPT-2 for sampling. Achieves a BPW of 5.76. Focuses on provable security **[ding2023discop]**. Key results include Capacity: 5.76, Entropy: 6.08, Utilization: 0.95."
  * **Instruction:** Append `[ding2023discop]` after the first sentence of the bullet point.
  * **Result:** "Discop (Provably Secure Steganography): Uses GPT-2 for sampling. Achieves a BPW of 5.76. Focuses on provable security **[ding2023discop]**. Key results include Capacity: 5.76, Entropy: 6.08, Utilization: 0.95."

* **Original Text in `main.pdf`:** "Generative Text Steganography with LLM: A black-box approach with a BPW of 5.93 and a PPL of 165.76. Focuses on semantic similarity **[wu2024generative]**. Key results include SS: 0.5881, LS-CNN Acc: 51.55"
  * **Instruction:** Append `[wu2024generative]` after the first sentence of the bullet point.
  * **Result:** "Generative Text Steganography with LLM: A black-box approach with a BPW of 5.93 and a PPL of 165.76. Focuses on semantic similarity **[wu2024generative]**. Key results include SS: 0.5881, LS-CNN Acc: 51.55"

* **Original Text in `main.pdf`:** "Meteor (Cryptographically Secure Steganography): Uses GPT-2 for sampling. Achieves 4.11 BPW. Focuses on cryptographic security **[kaptchuk2021meteor]**. Key result: 3.09 bits/token."
  * **Instruction:** Append `[kaptchuk2021meteor]` after the first sentence of the bullet point.
  * **Result:** "Meteor (Cryptographically Secure Steganography): Uses GPT-2 for sampling. Achieves 4.11 BPW. Focuses on cryptographic security **[kaptchuk2021meteor]**. Key result: 3.09 bits/token."}]}}}

* **Original Text in `main.pdf`:** "Zero-shot Generative Linguistic Steganography: Uses LLaMA2-Chat-7B and GPT-2. Achieves a BPW of 2.511 and a PPL of 8.81."
  * **Instruction:** Append `[lin2024zero]` after the first sentence of the bullet point.
  * **Result:** "Zero-shot Generative Linguistic Steganography: Uses LLaMA2-Chat-7B and GPT-2. Achieves a BPW of 2.511 and a PPL of 8.81 **[lin2024zero]**. Other metrics include JSDfull: 17.90, JSDhalf: 16.86, JSDzero: 13.40."

* **Original Text in `main.pdf`:** "Provably Secure Disambiguating Neural Linguistic Steganography: Uses LLaMA2-7b and Baichuan2-7b. Achieves 0.85 BPW with zero decoding error and provable security."
  * **Instruction:** Append `[qi2024provably]` after the first sentence of the bullet point.
  * **Result:** "Provably Secure Disambiguating Neural Linguistic Steganography: Uses LLaMA2-7b and Baichuan2-7b. Achieves 0.85 BPW with zero decoding error and provable security **[qi2024provably]**. Key results: Total Error: 0"

* **Original Text in `main.pdf`:** "A Principled Approach to Natural Language Watermarking: Transformer-based encoder/decoder; BERT for distillation. Achieves 0.2 BPW. Focuses on meaning preservation and robustness."
  * **Instruction:** Append `[ji2024principled]` after the first sentence of the bullet point.
  * **Result:** "A Principled Approach to Natural Language Watermarking: Transformer-based encoder/decoder; BERT for distillation. Achieves 0.2 BPW. Focuses on meaning preservation and robustness **[ji2024principled]**. Key results: Bit acc: 0.994, Meteor Drop: 0.057, SBERT ^: 1.227."

* **Original Text in `main.pdf`:** "Context-Aware Linguistic Steganography Model Based on Neural Machine Translation: Uses BERT (encoder) and LSTM (decoder). Achieves 3.275 BPW."
  * **Instruction:** Append `[ding2023context]` after the first sentence of the bullet point.
  * **Result:** "Context-Aware Linguistic Steganography Model Based on Neural Machine Translation: Uses BERT (encoder) and LSTM (decoder). Achieves 3.275 BPW **[ding2023context]**. Key results: BLEU: 30.5, PPL: 22.5, ER: 0.29, KL: 0.02, SIM: 0.86."

* **Original Text in `main.pdf`:** "DeepTextMark: Model-independent watermarking approach tested with OPT-2.7B. Achieves 1 BPW with high detection accuracy and robustness."
  * **Instruction:** Append `[munyer2024deeptextmark]` after the first sentence of the bullet point.
  * **Result:** "DeepTextMark: Model-independent watermarking approach tested with OPT-2.7B. Achieves 1 BPW with high detection accuracy and robustness **[munyer2024deeptextmark]**. Key results: 100"

* **Original Text in `main.pdf`:** "Hi-Stega: Hierarchical Linguistic Steganography Framework combining retrieval and generation usingGPT-2. Achieves 10.42 BPW with high payload and semantic coherence."
  * **Instruction:** Append `[wang2023hi]` after the first sentence of the bullet point.
  * **Result:** "Hi-Stega: Hierarchical Linguistic Steganography Framework combining retrieval and generation usingGPT-2. Achieves 10.42 BPW with high payload and semantic coherence **[wang2023hi]**. Key results: ppl: 109.60, MAUVE: 0.2051, ER2: 10.42."

* **Original Text in `main.pdf`:** "Linguistic Steganography: From Symbolic Space to Semantic Space: Uses CTRL (generation) and BERT (semantic classifier). Achieves 0.08 BPW."
  * **Instruction:** Append `[zhang2020linguistic]` after the first sentence of the bullet point.
  * **Result:** "Linguistic Steganography: From Symbolic Space to Semantic Space: Uses CTRL (generation) and BERT (semantic classifier). Achieves 0.08 BPW **[zhang2020linguistic]**. Key results: Classifier Accuracy: 0.9880, Loop Count: 1.0160, PPL: 13.9565."

* **Original Text in `main.pdf`:** "Natural Language Steganography by ChatGPT: Uses ChatGPT 4.0. Achieves 0.144 BPWwith natural concealment and scalability."
  * **Instruction:** Append `[steinebach2024natural]` after the sentence.
  * **Result:** "Natural Language Steganography by ChatGPT: Uses ChatGPT 4.0. Achieves 0.144 BPWwith natural concealment and scalability **[steinebach2024natural]**."

* **Original Text in `main.pdf`:** "Rewriting-Stego: Uses BART (bart-base2). Achieves 4 BPW with high capacity and naturalness."
  * **Instruction:** Append `[li2023rewriting]` after the first sentence of the bullet point.
  * **Result:** "Rewriting-Stego: Uses BART (bart-base2). Achieves 4 BPW with high capacity and naturalness **[li2023rewriting]**. Key results: BPTS: 4.0, BPTC+S: 4.0, PPL: 62.1."

* **Original Text in `main.pdf`:** "ALiSa (Acrostic Linguistic Steganography): Based on BERT and Gibbs Sampling. Achieves 0.92 BPW."
  * **Instruction:** Append `[yi2022alisa]` after the first sentence of the bullet point.
  * **Result:** "ALiSa (Acrostic Linguistic Steganography): Based on BERT and Gibbs Sampling. Achieves 0.92 BPW **[yi2022alisa]**. Key results: PPL: Natural = 13.91, ALiSa = 14.85, LS-RNN/LS-BERT Acc & F1 = 0.50."}]}}}
